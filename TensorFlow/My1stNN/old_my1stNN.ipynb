{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 28, 28) (50000,)\n",
      "(10000, 28, 28) (10000,)\n",
      "(10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "os.chdir(\"D:\\\\Downloads\\\\\")\n",
    "from preprocessed_mnist import load_dataset\n",
    "\n",
    "# load 'mnist' dataset of handwritten images (https://keras.io/datasets/)\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_dataset()\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "print(X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x12a800ab630>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12aa6dcfb38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.imshow(X_train[35], cmap=\"Greys\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = tf.Session()\n",
    "\n",
    "# Reload dataset with features in flat format\n",
    "X_train_flat, y_train, X_val_flat, y_val, X_test_flat, y_test = load_dataset(flatten=True)\n",
    "\n",
    "# Categorical labels to binaries\n",
    "y_train_oh = s.run(tf.one_hot(y_train, 10))\n",
    "y_test_oh  = s.run(tf.one_hot(y_test, 10))\n",
    "y_val_oh   = s.run(tf.one_hot(y_val, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'Placeholder_2:0' shape=(?, 784) dtype=float32>,\n",
       " <tf.Tensor 'Placeholder_3:0' shape=(?, 10) dtype=float32>,\n",
       " <tf.Variable 'weights_h_1:0' shape=(784, 50) dtype=float32_ref>,\n",
       " <tf.Variable 'weights_1:0' shape=(50, 10) dtype=float32_ref>,\n",
       " <tf.Variable 'biases_h_1:0' shape=(50,) dtype=float32_ref>,\n",
       " <tf.Variable 'biases_1:0' shape=(10,) dtype=float32_ref>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model parameters - weights and bias\n",
    "nhid   = 50\n",
    "nclass = len(np.unique(y_train))\n",
    "weights_hid = tf.Variable(tf.random_normal([X_train_flat.shape[1], nhid], stddev=0.35),\n",
    "                      name=\"weights_h\") \n",
    "\n",
    "b_hid = tf.Variable(tf.zeros([nhid]), dtype='float32', name=\"biases_h\")\n",
    "\n",
    "weights_out = tf.Variable(tf.random_normal([nhid, nclass], stddev=0.35),\n",
    "                      name=\"weights\") \n",
    "\n",
    "b_out = tf.Variable(tf.zeros([nclass]), dtype='float32', name=\"biases\")\n",
    "\n",
    "# Placeholders for the input data\n",
    "input_X = tf.placeholder('float32', shape=(None,X_train_flat.shape[1]))\n",
    "input_y = tf.placeholder('float32', shape=(None, nclass))\n",
    "input_X, input_y, weights_hid, weights_out, b_hid, b_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model\n",
    "#predicted_y =  tf.nn.softmax(tf.matmul(input_X, weights)+b)\n",
    "predicted_y_hid =  tf.nn.relu(tf.matmul(input_X, weights_hid)+b_hid)\n",
    "predicted_y     =  tf.matmul(predicted_y_hid, weights_out)+b_out\n",
    "\n",
    "# Loss. Should be a scalar number - average loss over all the objects\n",
    "#loss = tf.reduce_mean(-tf.reduce_sum(tf.log(predicted_y+1e-07)*input_y, reduction_indices=[1]))\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=input_y, logits=predicted_y))\n",
    "\n",
    "# See above for an example. tf.train.*Optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.5).minimize(\n",
    "    loss, var_list=(weights_hid, b_hid, weights_out, b_out))\n",
    "\n",
    "# compute accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(predicted_y,1), tf.argmax(input_y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at iter 0:6.2919\n",
      "train auc: 0.58335591091\n",
      "test auc: 0.587046529894\n",
      "loss at iter 1:3.6300\n",
      "train auc: 0.680346819807\n",
      "test auc: 0.684605617402\n",
      "loss at iter 2:2.1468\n",
      "train auc: 0.717883381394\n",
      "test auc: 0.723069321929\n",
      "loss at iter 3:1.8736\n",
      "train auc: 0.745649948611\n",
      "test auc: 0.750616156593\n",
      "loss at iter 4:1.7157\n",
      "train auc: 0.76964637616\n",
      "test auc: 0.77449417062\n",
      "loss at iter 5:1.5911\n",
      "train auc: 0.791106008856\n",
      "test auc: 0.795901123492\n",
      "loss at iter 6:1.4846\n",
      "train auc: 0.809992785558\n",
      "test auc: 0.814772644749\n",
      "loss at iter 7:1.3906\n",
      "train auc: 0.826191377866\n",
      "test auc: 0.830987511265\n",
      "loss at iter 8:1.3072\n",
      "train auc: 0.840029531966\n",
      "test auc: 0.844736250014\n",
      "loss at iter 9:1.2328\n",
      "train auc: 0.851826060772\n",
      "test auc: 0.856468604758\n",
      "loss at iter 10:1.1671\n",
      "train auc: 0.861823558819\n",
      "test auc: 0.866409542765\n",
      "loss at iter 11:1.1089\n",
      "train auc: 0.870376151842\n",
      "test auc: 0.874892079164\n",
      "loss at iter 12:1.0569\n",
      "train auc: 0.877674946403\n",
      "test auc: 0.882179540103\n",
      "loss at iter 13:1.0105\n",
      "train auc: 0.883922416177\n",
      "test auc: 0.888460493218\n",
      "loss at iter 14:0.9690\n",
      "train auc: 0.889326643935\n",
      "test auc: 0.893870066853\n",
      "loss at iter 15:0.9320\n",
      "train auc: 0.894015870683\n",
      "test auc: 0.898545477493\n",
      "loss at iter 16:0.8988\n",
      "train auc: 0.898103144397\n",
      "test auc: 0.902600390536\n",
      "loss at iter 17:0.8688\n",
      "train auc: 0.901680246534\n",
      "test auc: 0.906149982877\n",
      "loss at iter 18:0.8417\n",
      "train auc: 0.904831113911\n",
      "test auc: 0.909286200905\n",
      "loss at iter 19:0.8171\n",
      "train auc: 0.907625852714\n",
      "test auc: 0.91206776567\n",
      "loss at iter 20:0.7946\n",
      "train auc: 0.910118730223\n",
      "test auc: 0.914555251448\n",
      "loss at iter 21:0.7740\n",
      "train auc: 0.912352528278\n",
      "test auc: 0.916771480827\n",
      "loss at iter 22:0.7552\n",
      "train auc: 0.914369718707\n",
      "test auc: 0.918762918836\n",
      "loss at iter 23:0.7378\n",
      "train auc: 0.916194132475\n",
      "test auc: 0.920577452918\n",
      "loss at iter 24:0.7217\n",
      "train auc: 0.917852336858\n",
      "test auc: 0.922215671885\n",
      "loss at iter 25:0.7067\n",
      "train auc: 0.919373025244\n",
      "test auc: 0.92371713316\n",
      "loss at iter 26:0.6928\n",
      "train auc: 0.920775299788\n",
      "test auc: 0.925099193308\n",
      "loss at iter 27:0.6797\n",
      "train auc: 0.922071071309\n",
      "test auc: 0.926365963025\n",
      "loss at iter 28:0.6674\n",
      "train auc: 0.923269441908\n",
      "test auc: 0.927542105499\n",
      "loss at iter 29:0.6559\n",
      "train auc: 0.924378854\n",
      "test auc: 0.928633992931\n",
      "loss at iter 30:0.6450\n",
      "train auc: 0.92541974802\n",
      "test auc: 0.929649764981\n",
      "loss at iter 31:0.6348\n",
      "train auc: 0.926392339604\n",
      "test auc: 0.930585257072\n",
      "loss at iter 32:0.6251\n",
      "train auc: 0.927301485153\n",
      "test auc: 0.931467329705\n",
      "loss at iter 33:0.6160\n",
      "train auc: 0.928158907113\n",
      "test auc: 0.932291785621\n",
      "loss at iter 34:0.6073\n",
      "train auc: 0.9289611077\n",
      "test auc: 0.933062203028\n",
      "loss at iter 35:0.5991\n",
      "train auc: 0.929726158862\n",
      "test auc: 0.93379609491\n",
      "loss at iter 36:0.5913\n",
      "train auc: 0.930446930066\n",
      "test auc: 0.934483861084\n",
      "loss at iter 37:0.5838\n",
      "train auc: 0.931133962574\n",
      "test auc: 0.935143032642\n",
      "loss at iter 38:0.5767\n",
      "train auc: 0.931779063934\n",
      "test auc: 0.935764440228\n",
      "loss at iter 39:0.5699\n",
      "train auc: 0.932394371468\n",
      "test auc: 0.936343169616\n",
      "loss at iter 40:0.5634\n",
      "train auc: 0.932976611959\n",
      "test auc: 0.936900948929\n",
      "loss at iter 41:0.5572\n",
      "train auc: 0.933539519184\n",
      "test auc: 0.93743072177\n",
      "loss at iter 42:0.5512\n",
      "train auc: 0.934067927665\n",
      "test auc: 0.937933461844\n",
      "loss at iter 43:0.5455\n",
      "train auc: 0.934577855162\n",
      "test auc: 0.938417331868\n",
      "loss at iter 44:0.5400\n",
      "train auc: 0.935064312345\n",
      "test auc: 0.938877642818\n",
      "loss at iter 45:0.5347\n",
      "train auc: 0.935541439548\n",
      "test auc: 0.939328570938\n",
      "loss at iter 46:0.5297\n",
      "train auc: 0.935992151523\n",
      "test auc: 0.93975474967\n",
      "loss at iter 47:0.5248\n",
      "train auc: 0.93643268087\n",
      "test auc: 0.940171423998\n",
      "loss at iter 48:0.5201\n",
      "train auc: 0.936847162898\n",
      "test auc: 0.940563460332\n",
      "loss at iter 49:0.5155\n",
      "train auc: 0.93725652587\n",
      "test auc: 0.940946726971\n",
      "loss at iter 50:0.5111\n",
      "train auc: 0.937639167888\n",
      "test auc: 0.941315544664\n",
      "loss at iter 51:0.5070\n",
      "train auc: 0.938023797408\n",
      "test auc: 0.941674815866\n",
      "loss at iter 52:0.5028\n",
      "train auc: 0.938376349388\n",
      "test auc: 0.942015064388\n",
      "loss at iter 53:0.4990\n",
      "train auc: 0.938740072821\n",
      "test auc: 0.942356918243\n",
      "loss at iter 54:0.4951\n",
      "train auc: 0.939065791918\n",
      "test auc: 0.942671719729\n",
      "loss at iter 55:0.4916\n",
      "train auc: 0.939410206045\n",
      "test auc: 0.94298811356\n",
      "loss at iter 56:0.4879\n",
      "train auc: 0.939710249122\n",
      "test auc: 0.943283058869\n",
      "loss at iter 57:0.4848\n",
      "train auc: 0.940045436884\n",
      "test auc: 0.943585864387\n",
      "loss at iter 58:0.4814\n",
      "train auc: 0.940320466956\n",
      "test auc: 0.94386050853\n",
      "loss at iter 59:0.4789\n",
      "train auc: 0.940648198176\n",
      "test auc: 0.944148471968\n",
      "loss at iter 60:0.4758\n",
      "train auc: 0.940894392319\n",
      "test auc: 0.944401240343\n",
      "loss at iter 61:0.4743\n",
      "train auc: 0.941223767601\n",
      "test auc: 0.944675062973\n",
      "loss at iter 62:0.4718\n",
      "train auc: 0.941435117871\n",
      "test auc: 0.944907601371\n",
      "loss at iter 63:0.4721\n",
      "train auc: 0.941773254879\n",
      "test auc: 0.945173378852\n",
      "loss at iter 64:0.4710\n",
      "train auc: 0.941948452616\n",
      "test auc: 0.945390720851\n",
      "loss at iter 65:0.4749\n",
      "train auc: 0.942312312846\n",
      "test auc: 0.945648498967\n",
      "loss at iter 66:0.4766\n",
      "train auc: 0.942445012981\n",
      "test auc: 0.945851739408\n",
      "loss at iter 67:0.4876\n",
      "train auc: 0.94284976744\n",
      "test auc: 0.946103716058\n",
      "loss at iter 68:0.4928\n",
      "train auc: 0.942936291508\n",
      "test auc: 0.946287608167\n",
      "loss at iter 69:0.5137\n",
      "train auc: 0.943390327059\n",
      "test auc: 0.946524889877\n",
      "loss at iter 70:0.5143\n",
      "train auc: 0.943429701827\n",
      "test auc: 0.94669733884\n",
      "loss at iter 71:0.5345\n",
      "train auc: 0.943926955039\n",
      "test auc: 0.94691996859\n",
      "loss at iter 72:0.5118\n",
      "train auc: 0.94395314412\n",
      "test auc: 0.947085258794\n",
      "loss at iter 73:0.5130\n",
      "train auc: 0.944448356848\n",
      "test auc: 0.947337140706\n",
      "loss at iter 74:0.4819\n",
      "train auc: 0.94450350893\n",
      "test auc: 0.947517296675\n",
      "loss at iter 75:0.4743\n",
      "train auc: 0.944938648772\n",
      "test auc: 0.94777778259\n",
      "loss at iter 76:0.4547\n",
      "train auc: 0.945027539776\n",
      "test auc: 0.947961016297\n",
      "loss at iter 77:0.4494\n",
      "train auc: 0.945380380124\n",
      "test auc: 0.948211154174\n",
      "loss at iter 78:0.4395\n",
      "train auc: 0.945496863565\n",
      "test auc: 0.948389239602\n",
      "loss at iter 79:0.4367\n",
      "train auc: 0.945788717184\n",
      "test auc: 0.948613944914\n",
      "loss at iter 80:0.4311\n",
      "train auc: 0.945924188064\n",
      "test auc: 0.948782405663\n",
      "loss at iter 81:0.4293\n",
      "train auc: 0.946175501859\n",
      "test auc: 0.948986953616\n",
      "loss at iter 82:0.4255\n",
      "train auc: 0.94632205199\n",
      "test auc: 0.949153522792\n",
      "loss at iter 83:0.4240\n",
      "train auc: 0.946548659996\n",
      "test auc: 0.949343404131\n",
      "loss at iter 84:0.4211\n",
      "train auc: 0.94669969014\n",
      "test auc: 0.949506697975\n",
      "loss at iter 85:0.4197\n",
      "train auc: 0.946906892157\n",
      "test auc: 0.949688160839\n",
      "loss at iter 86:0.4172\n",
      "train auc: 0.947058936135\n",
      "test auc: 0.949842918071\n",
      "loss at iter 87:0.4158\n",
      "train auc: 0.947251754624\n",
      "test auc: 0.950013632859\n",
      "loss at iter 88:0.4136\n",
      "train auc: 0.947400155756\n",
      "test auc: 0.950159819636\n",
      "loss at iter 89:0.4122\n",
      "train auc: 0.947583743576\n",
      "test auc: 0.950322459085\n",
      "loss at iter 90:0.4102\n",
      "train auc: 0.947730914763\n",
      "test auc: 0.950465096434\n",
      "loss at iter 91:0.4088\n",
      "train auc: 0.947904888093\n",
      "test auc: 0.950621293887\n",
      "loss at iter 92:0.4069\n",
      "train auc: 0.948047425343\n",
      "test auc: 0.950759364866\n",
      "loss at iter 93:0.4056\n",
      "train auc: 0.948210799624\n",
      "test auc: 0.950910369222\n",
      "loss at iter 94:0.4038\n",
      "train auc: 0.948349390944\n",
      "test auc: 0.951043135626\n",
      "loss at iter 95:0.4025\n",
      "train auc: 0.948506427636\n",
      "test auc: 0.951186545792\n",
      "loss at iter 96:0.4008\n",
      "train auc: 0.948642435901\n",
      "test auc: 0.951317023416\n",
      "loss at iter 97:0.3995\n",
      "train auc: 0.948793444998\n",
      "test auc: 0.951457317557\n",
      "loss at iter 98:0.3979\n",
      "train auc: 0.948925695911\n",
      "test auc: 0.951582625622\n",
      "loss at iter 99:0.3966\n",
      "train auc: 0.94907272757\n",
      "test auc: 0.951716787395\n",
      "loss at iter 100:0.3951\n",
      "train auc: 0.949200549504\n",
      "test auc: 0.951835272505\n",
      "loss at iter 101:0.3939\n",
      "train auc: 0.949339996803\n",
      "test auc: 0.951964551445\n",
      "loss at iter 102:0.3924\n",
      "train auc: 0.949462915361\n",
      "test auc: 0.952080599415\n",
      "loss at iter 103:0.3912\n",
      "train auc: 0.949598394824\n",
      "test auc: 0.952205574459\n",
      "loss at iter 104:0.3898\n",
      "train auc: 0.949718225945\n",
      "test auc: 0.952319159096\n",
      "loss at iter 105:0.3887\n",
      "train auc: 0.949851258759\n",
      "test auc: 0.952440426377\n",
      "loss at iter 106:0.3873\n",
      "train auc: 0.949969385013\n",
      "test auc: 0.952551997535\n",
      "loss at iter 107:0.3862\n",
      "train auc: 0.950099667163\n",
      "test auc: 0.952670790665\n",
      "loss at iter 108:0.3849\n",
      "train auc: 0.950215156912\n",
      "test auc: 0.952776778597\n",
      "loss at iter 109:0.3838\n",
      "train auc: 0.950343903523\n",
      "test auc: 0.952893500482\n",
      "loss at iter 110:0.3825\n",
      "train auc: 0.950456687244\n",
      "test auc: 0.953000013412\n",
      "loss at iter 111:0.3815\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train auc: 0.950582928285\n",
      "test auc: 0.953115404124\n",
      "loss at iter 112:0.3802\n",
      "train auc: 0.950692471162\n",
      "test auc: 0.953216606926\n",
      "loss at iter 113:0.3793\n",
      "train auc: 0.950815426362\n",
      "test auc: 0.95332775458\n",
      "loss at iter 114:0.3780\n",
      "train auc: 0.950920196996\n",
      "test auc: 0.953422903917\n",
      "loss at iter 115:0.3771\n",
      "train auc: 0.951041040743\n",
      "test auc: 0.953531392824\n",
      "loss at iter 116:0.3758\n",
      "train auc: 0.951141937272\n",
      "test auc: 0.953625936927\n",
      "loss at iter 117:0.3749\n",
      "train auc: 0.951259661168\n",
      "test auc: 0.953732300487\n",
      "loss at iter 118:0.3737\n",
      "train auc: 0.951357446892\n",
      "test auc: 0.953823452939\n",
      "loss at iter 119:0.3729\n",
      "train auc: 0.951473923126\n",
      "test auc: 0.953925390052\n",
      "loss at iter 120:0.3717\n",
      "train auc: 0.951568132255\n",
      "test auc: 0.954011289046\n",
      "loss at iter 121:0.3709\n",
      "train auc: 0.951680903071\n",
      "test auc: 0.954115455867\n",
      "loss at iter 122:0.3697\n",
      "train auc: 0.951771726001\n",
      "test auc: 0.954196805971\n",
      "loss at iter 123:0.3689\n",
      "train auc: 0.951884936448\n",
      "test auc: 0.954297921269\n",
      "loss at iter 124:0.3677\n",
      "train auc: 0.95197304163\n",
      "test auc: 0.954380050759\n",
      "loss at iter 125:0.3670\n",
      "train auc: 0.952085863025\n",
      "test auc: 0.95448185898\n",
      "loss at iter 126:0.3658\n",
      "train auc: 0.952170558979\n",
      "test auc: 0.954562510217\n",
      "loss at iter 127:0.3652\n",
      "train auc: 0.952284502424\n",
      "test auc: 0.954664523256\n",
      "loss at iter 128:0.3640\n",
      "train auc: 0.952365105211\n",
      "test auc: 0.954740948526\n",
      "loss at iter 129:0.3634\n",
      "train auc: 0.95248016621\n",
      "test auc: 0.954842258748\n",
      "loss at iter 130:0.3622\n",
      "train auc: 0.952556486164\n",
      "test auc: 0.954915311473\n",
      "loss at iter 131:0.3617\n",
      "train auc: 0.95267218699\n",
      "test auc: 0.955017313891\n",
      "loss at iter 132:0.3604\n",
      "train auc: 0.95274351524\n",
      "test auc: 0.95508327549\n",
      "loss at iter 133:0.3601\n",
      "train auc: 0.952860987944\n",
      "test auc: 0.955186252694\n",
      "loss at iter 134:0.3588\n",
      "train auc: 0.952925541083\n",
      "test auc: 0.955247401353\n",
      "loss at iter 135:0.3585\n",
      "train auc: 0.953046163598\n",
      "test auc: 0.95535330275\n",
      "loss at iter 136:0.3572\n",
      "train auc: 0.95310405607\n",
      "test auc: 0.955409298492\n",
      "loss at iter 137:0.3571\n",
      "train auc: 0.953230223818\n",
      "test auc: 0.955519459843\n",
      "loss at iter 138:0.3557\n",
      "train auc: 0.953279090075\n",
      "test auc: 0.955567728703\n",
      "loss at iter 139:0.3558\n",
      "train auc: 0.953412591169\n",
      "test auc: 0.955680978131\n",
      "loss at iter 140:0.3543\n",
      "train auc: 0.953451956598\n",
      "test auc: 0.955725873635\n",
      "loss at iter 141:0.3547\n",
      "train auc: 0.953595082604\n",
      "test auc: 0.955846090238\n",
      "loss at iter 142:0.3531\n",
      "train auc: 0.953623221452\n",
      "test auc: 0.95588368005\n",
      "loss at iter 143:0.3538\n",
      "train auc: 0.953777062024\n",
      "test auc: 0.956012518248\n",
      "loss at iter 144:0.3521\n",
      "train auc: 0.953792458963\n",
      "test auc: 0.956039154834\n",
      "loss at iter 145:0.3534\n",
      "train auc: 0.953961845061\n",
      "test auc: 0.956176113697\n",
      "loss at iter 146:0.3516\n",
      "train auc: 0.953959573672\n",
      "test auc: 0.95619204199\n",
      "loss at iter 147:0.3535\n",
      "train auc: 0.954148495681\n",
      "test auc: 0.956338183328\n",
      "loss at iter 148:0.3517\n",
      "train auc: 0.954126191669\n",
      "test auc: 0.956343726238\n",
      "loss at iter 149:0.3546\n",
      "train auc: 0.954340664562\n",
      "test auc: 0.956506267077\n",
      "loss at iter 150:0.3527\n",
      "train auc: 0.954296792424\n",
      "test auc: 0.956498074971\n",
      "loss at iter 151:0.3569\n",
      "train auc: 0.954543638584\n",
      "test auc: 0.956680682741\n",
      "loss at iter 152:0.3552\n",
      "train auc: 0.954474428448\n",
      "test auc: 0.956652763644\n",
      "loss at iter 153:0.3611\n",
      "train auc: 0.954762042458\n",
      "test auc: 0.956867146444\n",
      "loss at iter 154:0.3595\n",
      "train auc: 0.954668306974\n",
      "test auc: 0.956822703279\n",
      "loss at iter 155:0.3674\n",
      "train auc: 0.955003777507\n",
      "test auc: 0.957069189117\n",
      "loss at iter 156:0.3658\n",
      "train auc: 0.954886874161\n",
      "test auc: 0.957014790365\n",
      "loss at iter 157:0.3754\n",
      "train auc: 0.955267121041\n",
      "test auc: 0.957290527182\n",
      "loss at iter 158:0.3730\n",
      "train auc: 0.955138369619\n",
      "test auc: 0.957231898385\n",
      "loss at iter 159:0.3827\n",
      "train auc: 0.955553314348\n",
      "test auc: 0.957520609196\n",
      "loss at iter 160:0.3776\n",
      "train auc: 0.955425324204\n",
      "test auc: 0.957472227633\n",
      "loss at iter 161:0.3847\n",
      "train auc: 0.95584588947\n",
      "test auc: 0.957757423264\n",
      "loss at iter 162:0.3753\n",
      "train auc: 0.955734631544\n",
      "test auc: 0.957731091776\n",
      "loss at iter 163:0.3778\n",
      "train auc: 0.956122516555\n",
      "test auc: 0.957996238978\n",
      "loss at iter 164:0.3658\n",
      "train auc: 0.956030158493\n",
      "test auc: 0.957983252641\n",
      "loss at iter 165:0.3649\n",
      "train auc: 0.956353648232\n",
      "test auc: 0.958203165088\n",
      "loss at iter 166:0.3540\n",
      "train auc: 0.956281395671\n",
      "test auc: 0.958196874953\n",
      "loss at iter 167:0.3526\n",
      "train auc: 0.956530391747\n",
      "test auc: 0.958374281493\n",
      "loss at iter 168:0.3449\n",
      "train auc: 0.956479623979\n",
      "test auc: 0.958364748175\n",
      "loss at iter 169:0.3441\n",
      "train auc: 0.956667848487\n",
      "test auc: 0.958514143323\n",
      "loss at iter 170:0.3391\n",
      "train auc: 0.956636507146\n",
      "test auc: 0.958507875834\n",
      "loss at iter 171:0.3389\n",
      "train auc: 0.956782209322\n",
      "test auc: 0.958627783215\n",
      "loss at iter 172:0.3355\n",
      "train auc: 0.956770553759\n",
      "test auc: 0.958630924397\n",
      "loss at iter 173:0.3355\n",
      "train auc: 0.956887611279\n",
      "test auc: 0.958727902979\n",
      "loss at iter 174:0.3332\n",
      "train auc: 0.95689034486\n",
      "test auc: 0.958739439661\n",
      "loss at iter 175:0.3332\n",
      "train auc: 0.956989708408\n",
      "test auc: 0.958821862375\n",
      "loss at iter 176:0.3314\n",
      "train auc: 0.957004261802\n",
      "test auc: 0.958841879338\n",
      "loss at iter 177:0.3313\n",
      "train auc: 0.957091223435\n",
      "test auc: 0.958918119552\n",
      "loss at iter 178:0.3299\n",
      "train auc: 0.957113393833\n",
      "test auc: 0.958945061785\n",
      "loss at iter 179:0.3298\n",
      "train auc: 0.957192987797\n",
      "test auc: 0.959014513305\n",
      "loss at iter 180:0.3286\n",
      "train auc: 0.957221340565\n",
      "test auc: 0.959045229136\n",
      "loss at iter 181:0.3284\n",
      "train auc: 0.957295649996\n",
      "test auc: 0.959111498268\n",
      "loss at iter 182:0.3274\n",
      "train auc: 0.957327677342\n",
      "test auc: 0.959145086271\n",
      "loss at iter 183:0.3271\n",
      "train auc: 0.957398476857\n",
      "test auc: 0.95920745678\n",
      "loss at iter 184:0.3262\n",
      "train auc: 0.957434700677\n",
      "test auc: 0.959243222935\n",
      "loss at iter 185:0.3259\n",
      "train auc: 0.957500858106\n",
      "test auc: 0.959302977992\n",
      "loss at iter 186:0.3251\n",
      "train auc: 0.957539338099\n",
      "test auc: 0.95933950335\n",
      "loss at iter 187:0.3248\n",
      "train auc: 0.957602599471\n",
      "test auc: 0.959395866538\n",
      "loss at iter 188:0.3240\n",
      "train auc: 0.957642955056\n",
      "test auc: 0.959433788773\n",
      "loss at iter 189:0.3237\n",
      "train auc: 0.957704030286\n",
      "test auc: 0.959487865752\n",
      "loss at iter 190:0.3230\n",
      "train auc: 0.957745507858\n",
      "test auc: 0.959526526905\n",
      "loss at iter 191:0.3226\n",
      "train auc: 0.957805627698\n",
      "test auc: 0.959580170136\n",
      "loss at iter 192:0.3219\n",
      "train auc: 0.95784813331\n",
      "test auc: 0.95962165052\n",
      "loss at iter 193:0.3215\n",
      "train auc: 0.957906003086\n",
      "test auc: 0.959674419896\n",
      "loss at iter 194:0.3209\n",
      "train auc: 0.957949208799\n",
      "test auc: 0.959716944594\n",
      "loss at iter 195:0.3205\n",
      "train auc: 0.958006210838\n",
      "test auc: 0.959766463935\n",
      "loss at iter 196:0.3199\n",
      "train auc: 0.958050947531\n",
      "test auc: 0.959811058203\n",
      "loss at iter 197:0.3195\n",
      "train auc: 0.958107721549\n",
      "test auc: 0.959860077236\n",
      "loss at iter 198:0.3189\n",
      "train auc: 0.958153361502\n",
      "test auc: 0.959903294766\n",
      "loss at iter 199:0.3185\n",
      "train auc: 0.9582093521\n",
      "test auc: 0.959950828926\n",
      "loss at iter 200:0.3179\n",
      "train auc: 0.95825519807\n",
      "test auc: 0.959993690932\n",
      "loss at iter 201:0.3176\n",
      "train auc: 0.958310935144\n",
      "test auc: 0.960042307932\n",
      "loss at iter 202:0.3170\n",
      "train auc: 0.958355593437\n",
      "test auc: 0.960084692828\n",
      "loss at iter 203:0.3166\n",
      "train auc: 0.958409199161\n",
      "test auc: 0.960132549688\n",
      "loss at iter 204:0.3161\n",
      "train auc: 0.958454690699\n",
      "test auc: 0.96017509856\n",
      "loss at iter 205:0.3157\n",
      "train auc: 0.958507494636\n",
      "test auc: 0.960222316557\n",
      "loss at iter 206:0.3151\n",
      "train auc: 0.958552411842\n",
      "test auc: 0.960263013314\n",
      "loss at iter 207:0.3148\n",
      "train auc: 0.958603905877\n",
      "test auc: 0.960309819191\n",
      "loss at iter 208:0.3142\n",
      "train auc: 0.958647924413\n",
      "test auc: 0.960349514948\n",
      "loss at iter 209:0.3139\n",
      "train auc: 0.958698288257\n",
      "test auc: 0.960394466701\n",
      "loss at iter 210:0.3133\n",
      "train auc: 0.958742529822\n",
      "test auc: 0.960435693838\n",
      "loss at iter 211:0.3130\n",
      "train auc: 0.958792760951\n",
      "test auc: 0.960479617689\n",
      "loss at iter 212:0.3125\n",
      "train auc: 0.958837714871\n",
      "test auc: 0.960523334602\n",
      "loss at iter 213:0.3121\n",
      "train auc: 0.958887714152\n",
      "test auc: 0.96056794862\n",
      "loss at iter 214:0.3116\n",
      "train auc: 0.958931208428\n",
      "test auc: 0.96061009889\n",
      "loss at iter 215:0.3112\n",
      "train auc: 0.95898008465\n",
      "test auc: 0.960653712704\n",
      "loss at iter 216:0.3107\n",
      "train auc: 0.95902321343\n",
      "test auc: 0.960693937634\n",
      "loss at iter 217:0.3104\n",
      "train auc: 0.959071816915\n",
      "test auc: 0.960737528584\n",
      "loss at iter 218:0.3099\n",
      "train auc: 0.959114837686\n",
      "test auc: 0.960778575765\n",
      "loss at iter 219:0.3095\n",
      "train auc: 0.959162489951\n",
      "test auc: 0.960821933973\n",
      "loss at iter 220:0.3091\n",
      "train auc: 0.95920572528\n",
      "test auc: 0.960861526213\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at iter 221:0.3087\n",
      "train auc: 0.959253206884\n",
      "test auc: 0.960905992093\n",
      "loss at iter 222:0.3082\n",
      "train auc: 0.959295702902\n",
      "test auc: 0.960944599323\n",
      "loss at iter 223:0.3079\n",
      "train auc: 0.959343057482\n",
      "test auc: 0.960986993086\n",
      "loss at iter 224:0.3074\n",
      "train auc: 0.959385639704\n",
      "test auc: 0.961024915418\n",
      "loss at iter 225:0.3071\n",
      "train auc: 0.959431708649\n",
      "test auc: 0.961065271162\n",
      "loss at iter 226:0.3066\n",
      "train auc: 0.959473443925\n",
      "test auc: 0.961103120236\n",
      "loss at iter 227:0.3063\n",
      "train auc: 0.959519069873\n",
      "test auc: 0.961141522756\n",
      "loss at iter 228:0.3059\n",
      "train auc: 0.959559890655\n",
      "test auc: 0.961177503132\n",
      "loss at iter 229:0.3055\n",
      "train auc: 0.959605081027\n",
      "test auc: 0.961217869777\n",
      "loss at iter 230:0.3051\n",
      "train auc: 0.95964604679\n",
      "test auc: 0.9612541784\n",
      "loss at iter 231:0.3047\n",
      "train auc: 0.959691667263\n",
      "test auc: 0.961296191892\n",
      "loss at iter 232:0.3043\n",
      "train auc: 0.959732803608\n",
      "test auc: 0.961333271566\n",
      "loss at iter 233:0.3040\n",
      "train auc: 0.959778027615\n",
      "test auc: 0.961372798121\n",
      "loss at iter 234:0.3036\n",
      "train auc: 0.959818114363\n",
      "test auc: 0.961408887208\n",
      "loss at iter 235:0.3032\n",
      "train auc: 0.959862885203\n",
      "test auc: 0.961448701616\n",
      "loss at iter 236:0.3028\n",
      "train auc: 0.959902956671\n",
      "test auc: 0.961485279822\n",
      "loss at iter 237:0.3025\n",
      "train auc: 0.959947552724\n",
      "test auc: 0.961523070825\n",
      "loss at iter 238:0.3021\n",
      "train auc: 0.959986819374\n",
      "test auc: 0.96155982905\n",
      "loss at iter 239:0.3017\n",
      "train auc: 0.960031046392\n",
      "test auc: 0.961597799182\n",
      "loss at iter 240:0.3013\n",
      "train auc: 0.960070197631\n",
      "test auc: 0.961633662251\n",
      "loss at iter 241:0.3010\n",
      "train auc: 0.960114387479\n",
      "test auc: 0.961672155286\n",
      "loss at iter 242:0.3006\n",
      "train auc: 0.960153095225\n",
      "test auc: 0.961707304762\n",
      "loss at iter 243:0.3003\n",
      "train auc: 0.960196580547\n",
      "test auc: 0.961744819817\n",
      "loss at iter 244:0.2999\n",
      "train auc: 0.960234973533\n",
      "test auc: 0.961779828556\n",
      "loss at iter 245:0.2996\n",
      "train auc: 0.960278885255\n",
      "test auc: 0.961820177303\n",
      "loss at iter 246:0.2992\n",
      "train auc: 0.960316855852\n",
      "test auc: 0.961856498542\n",
      "loss at iter 247:0.2989\n",
      "train auc: 0.960360150317\n",
      "test auc: 0.961895622719\n",
      "loss at iter 248:0.2985\n",
      "train auc: 0.960398318494\n",
      "test auc: 0.961930495689\n",
      "loss at iter 249:0.2982\n",
      "train auc: 0.960441248393\n",
      "test auc: 0.961969306754\n",
      "loss at iter 250:0.2978\n",
      "train auc: 0.96047842351\n",
      "test auc: 0.962004009842\n",
      "loss at iter 251:0.2975\n",
      "train auc: 0.960521055468\n",
      "test auc: 0.962040159005\n",
      "loss at iter 252:0.2971\n",
      "train auc: 0.960558106193\n",
      "test auc: 0.96207360254\n",
      "loss at iter 253:0.2969\n",
      "train auc: 0.960600875921\n",
      "test auc: 0.962111795082\n",
      "loss at iter 254:0.2965\n",
      "train auc: 0.960637601508\n",
      "test auc: 0.962145575755\n",
      "loss at iter 255:0.2962\n",
      "train auc: 0.960679992144\n",
      "test auc: 0.962182458015\n",
      "loss at iter 256:0.2958\n",
      "train auc: 0.960716154954\n",
      "test auc: 0.962215935738\n",
      "loss at iter 257:0.2955\n",
      "train auc: 0.960758216461\n",
      "test auc: 0.962253734494\n",
      "loss at iter 258:0.2951\n",
      "train auc: 0.960793585181\n",
      "test auc: 0.962287566215\n",
      "loss at iter 259:0.2949\n",
      "train auc: 0.960835428372\n",
      "test auc: 0.962324396151\n",
      "loss at iter 260:0.2945\n",
      "train auc: 0.960871119102\n",
      "test auc: 0.962355920102\n",
      "loss at iter 261:0.2942\n",
      "train auc: 0.960913115355\n",
      "test auc: 0.962392703183\n",
      "loss at iter 262:0.2938\n",
      "train auc: 0.96094832137\n",
      "test auc: 0.962426014881\n",
      "loss at iter 263:0.2936\n",
      "train auc: 0.960990490206\n",
      "test auc: 0.962462008094\n",
      "loss at iter 264:0.2932\n",
      "train auc: 0.961024926606\n",
      "test auc: 0.962492770366\n",
      "loss at iter 265:0.2929\n",
      "train auc: 0.961066815979\n",
      "test auc: 0.962529816564\n",
      "loss at iter 266:0.2925\n",
      "train auc: 0.961101710169\n",
      "test auc: 0.96256252968\n",
      "loss at iter 267:0.2923\n",
      "train auc: 0.961143860071\n",
      "test auc: 0.962598610499\n",
      "loss at iter 268:0.2919\n",
      "train auc: 0.961177946682\n",
      "test auc: 0.962632223994\n",
      "loss at iter 269:0.2917\n",
      "train auc: 0.961219958941\n",
      "test auc: 0.962667133271\n",
      "loss at iter 270:0.2913\n",
      "train auc: 0.961253305841\n",
      "test auc: 0.962697299805\n",
      "loss at iter 271:0.2911\n",
      "train auc: 0.961295213362\n",
      "test auc: 0.962732523324\n",
      "loss at iter 272:0.2907\n",
      "train auc: 0.961328082225\n",
      "test auc: 0.962764464444\n",
      "loss at iter 273:0.2905\n",
      "train auc: 0.961369759981\n",
      "test auc: 0.962799680963\n",
      "loss at iter 274:0.2901\n",
      "train auc: 0.961401466784\n",
      "test auc: 0.962830261068\n",
      "loss at iter 275:0.2898\n",
      "train auc: 0.961443084209\n",
      "test auc: 0.962867099519\n",
      "loss at iter 276:0.2894\n",
      "train auc: 0.961473962429\n",
      "test auc: 0.962895121556\n",
      "loss at iter 277:0.2892\n",
      "train auc: 0.961514492222\n",
      "test auc: 0.96292886004\n",
      "loss at iter 278:0.2888\n",
      "train auc: 0.961543615297\n",
      "test auc: 0.962955217153\n",
      "loss at iter 279:0.2887\n",
      "train auc: 0.961584342956\n",
      "test auc: 0.962990381516\n",
      "loss at iter 280:0.2882\n",
      "train auc: 0.961613725144\n",
      "test auc: 0.963017715996\n",
      "loss at iter 281:0.2881\n",
      "train auc: 0.961654338414\n",
      "test auc: 0.96305250213\n",
      "loss at iter 282:0.2877\n",
      "train auc: 0.961683252326\n",
      "test auc: 0.96307925938\n",
      "loss at iter 283:0.2875\n",
      "train auc: 0.961724033549\n",
      "test auc: 0.963115192848\n",
      "loss at iter 284:0.2871\n",
      "train auc: 0.961752828524\n",
      "test auc: 0.9631414802\n",
      "loss at iter 285:0.2869\n",
      "train auc: 0.9617939972\n",
      "test auc: 0.963176034296\n",
      "loss at iter 286:0.2865\n",
      "train auc: 0.961822049521\n",
      "test auc: 0.963203233495\n",
      "loss at iter 287:0.2863\n",
      "train auc: 0.961863192958\n",
      "test auc: 0.963237042734\n",
      "loss at iter 288:0.2859\n",
      "train auc: 0.961890481763\n",
      "test auc: 0.963262208801\n",
      "loss at iter 289:0.2858\n",
      "train auc: 0.961931492822\n",
      "test auc: 0.963297701761\n",
      "loss at iter 290:0.2853\n",
      "train auc: 0.961958323032\n",
      "test auc: 0.963323328208\n",
      "loss at iter 291:0.2852\n",
      "train auc: 0.961999402907\n",
      "test auc: 0.963357082463\n",
      "loss at iter 292:0.2848\n",
      "train auc: 0.962025792193\n",
      "test auc: 0.963382578386\n",
      "loss at iter 293:0.2846\n",
      "train auc: 0.962066846238\n",
      "test auc: 0.963415470097\n",
      "loss at iter 294:0.2842\n",
      "train auc: 0.962092773576\n",
      "test auc: 0.963440994581\n",
      "loss at iter 295:0.2841\n",
      "train auc: 0.962134901121\n",
      "test auc: 0.963476585006\n",
      "loss at iter 296:0.2836\n",
      "train auc: 0.962160310475\n",
      "test auc: 0.963502849826\n",
      "loss at iter 297:0.2835\n",
      "train auc: 0.962202144725\n",
      "test auc: 0.963536541833\n",
      "loss at iter 298:0.2831\n",
      "train auc: 0.962227046155\n",
      "test auc: 0.963563762328\n",
      "loss at iter 299:0.2830\n",
      "train auc: 0.96226946213\n",
      "test auc: 0.963598258335\n",
      "loss at iter 300:0.2825\n",
      "train auc: 0.962293090454\n",
      "test auc: 0.963621798766\n",
      "loss at iter 301:0.2824\n",
      "train auc: 0.962334997703\n",
      "test auc: 0.963654956819\n",
      "loss at iter 302:0.2820\n",
      "train auc: 0.962357990704\n",
      "test auc: 0.963679145287\n",
      "loss at iter 303:0.2819\n",
      "train auc: 0.962400329032\n",
      "test auc: 0.963715896683\n",
      "loss at iter 304:0.2814\n",
      "train auc: 0.962422258879\n",
      "test auc: 0.963737783468\n",
      "loss at iter 305:0.2814\n",
      "train auc: 0.962464914635\n",
      "test auc: 0.963774584371\n",
      "loss at iter 306:0.2809\n",
      "train auc: 0.962486405522\n",
      "test auc: 0.963796336561\n",
      "loss at iter 307:0.2809\n",
      "train auc: 0.962528904363\n",
      "test auc: 0.963832352681\n",
      "loss at iter 308:0.2804\n",
      "train auc: 0.962549068237\n",
      "test auc: 0.963852942194\n",
      "loss at iter 309:0.2803\n",
      "train auc: 0.962592205406\n",
      "test auc: 0.963887819487\n",
      "loss at iter 310:0.2798\n",
      "train auc: 0.962611326231\n",
      "test auc: 0.963906489439\n",
      "loss at iter 311:0.2798\n",
      "train auc: 0.962654706966\n",
      "test auc: 0.963942087326\n",
      "loss at iter 312:0.2793\n",
      "train auc: 0.962672746521\n",
      "test auc: 0.963960844888\n",
      "loss at iter 313:0.2793\n",
      "train auc: 0.962716274405\n",
      "test auc: 0.963994936656\n",
      "loss at iter 314:0.2788\n",
      "train auc: 0.962733140504\n",
      "test auc: 0.964013409691\n",
      "loss at iter 315:0.2788\n",
      "train auc: 0.962777117861\n",
      "test auc: 0.964048376783\n",
      "loss at iter 316:0.2783\n",
      "train auc: 0.962793042757\n",
      "test auc: 0.964067623462\n",
      "loss at iter 317:0.2783\n",
      "train auc: 0.962837357625\n",
      "test auc: 0.964101375128\n",
      "loss at iter 318:0.2778\n",
      "train auc: 0.962852616305\n",
      "test auc: 0.964120886015\n",
      "loss at iter 319:0.2778\n",
      "train auc: 0.962896335005\n",
      "test auc: 0.964154389099\n",
      "loss at iter 320:0.2773\n",
      "train auc: 0.962910390914\n",
      "test auc: 0.964172214905\n",
      "loss at iter 321:0.2773\n",
      "train auc: 0.962955221378\n",
      "test auc: 0.9642067385\n",
      "loss at iter 322:0.2768\n",
      "train auc: 0.962968613775\n",
      "test auc: 0.964222481895\n",
      "loss at iter 323:0.2768\n",
      "train auc: 0.963013543381\n",
      "test auc: 0.964258507054\n",
      "loss at iter 324:0.2763\n",
      "train auc: 0.963026396474\n",
      "test auc: 0.964273626333\n",
      "loss at iter 325:0.2763\n",
      "train auc: 0.963071765797\n",
      "test auc: 0.964309483586\n",
      "loss at iter 326:0.2758\n",
      "train auc: 0.96308445054\n",
      "test auc: 0.964324456899\n",
      "loss at iter 327:0.2759\n",
      "train auc: 0.963130333953\n",
      "test auc: 0.964361338245\n",
      "loss at iter 328:0.2753\n",
      "train auc: 0.963141656525\n",
      "test auc: 0.964375491463\n",
      "loss at iter 329:0.2754\n",
      "train auc: 0.963187937855\n",
      "test auc: 0.964411707229\n",
      "loss at iter 330:0.2748\n",
      "train auc: 0.963197961286\n",
      "test auc: 0.964425344892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at iter 331:0.2749\n",
      "train auc: 0.963244418456\n",
      "test auc: 0.964462370481\n",
      "loss at iter 332:0.2743\n",
      "train auc: 0.963254265271\n",
      "test auc: 0.964476475928\n",
      "loss at iter 333:0.2744\n",
      "train auc: 0.963300755454\n",
      "test auc: 0.964511875751\n",
      "loss at iter 334:0.2738\n",
      "train auc: 0.963309828187\n",
      "test auc: 0.964526598573\n",
      "loss at iter 335:0.2740\n",
      "train auc: 0.963356910346\n",
      "test auc: 0.964561645599\n",
      "loss at iter 336:0.2734\n",
      "train auc: 0.963365628516\n",
      "test auc: 0.964577418594\n",
      "loss at iter 337:0.2735\n",
      "train auc: 0.963413066279\n",
      "test auc: 0.964612466721\n",
      "loss at iter 338:0.2729\n",
      "train auc: 0.963420731015\n",
      "test auc: 0.964626784593\n",
      "loss at iter 339:0.2730\n",
      "train auc: 0.963468529877\n",
      "test auc: 0.964659821422\n",
      "loss at iter 340:0.2724\n",
      "train auc: 0.96347587959\n",
      "test auc: 0.964676183169\n",
      "loss at iter 341:0.2726\n",
      "train auc: 0.963524530786\n",
      "test auc: 0.964710675912\n",
      "loss at iter 342:0.2719\n",
      "train auc: 0.963530965969\n",
      "test auc: 0.964727245809\n",
      "loss at iter 343:0.2721\n",
      "train auc: 0.963580360104\n",
      "test auc: 0.964762796777\n",
      "loss at iter 344:0.2715\n",
      "train auc: 0.963586559425\n",
      "test auc: 0.964777197883\n",
      "loss at iter 345:0.2717\n",
      "train auc: 0.96363536738\n",
      "test auc: 0.964812110454\n",
      "loss at iter 346:0.2710\n",
      "train auc: 0.963640950134\n",
      "test auc: 0.964826193181\n",
      "loss at iter 347:0.2712\n",
      "train auc: 0.963690254028\n",
      "test auc: 0.964861284165\n",
      "loss at iter 348:0.2706\n",
      "train auc: 0.96369488377\n",
      "test auc: 0.96487332927\n",
      "loss at iter 349:0.2708\n",
      "train auc: 0.963744948726\n",
      "test auc: 0.964909974326\n",
      "loss at iter 350:0.2701\n",
      "train auc: 0.963748794073\n",
      "test auc: 0.964919697925\n",
      "loss at iter 351:0.2704\n",
      "train auc: 0.963799196648\n",
      "test auc: 0.964958079214\n",
      "loss at iter 352:0.2697\n",
      "train auc: 0.963802064529\n",
      "test auc: 0.964967307089\n",
      "loss at iter 353:0.2699\n",
      "train auc: 0.96385266761\n",
      "test auc: 0.965005067725\n",
      "loss at iter 354:0.2692\n",
      "train auc: 0.963854828654\n",
      "test auc: 0.965015046373\n",
      "loss at iter 355:0.2695\n",
      "train auc: 0.963905552427\n",
      "test auc: 0.965052921875\n",
      "loss at iter 356:0.2688\n",
      "train auc: 0.963907050881\n",
      "test auc: 0.965064022312\n",
      "loss at iter 357:0.2691\n",
      "train auc: 0.963958389651\n",
      "test auc: 0.965100671273\n",
      "loss at iter 358:0.2684\n",
      "train auc: 0.963959788694\n",
      "test auc: 0.965111311479\n",
      "loss at iter 359:0.2686\n",
      "train auc: 0.964011091749\n",
      "test auc: 0.96514632018\n",
      "loss at iter 360:0.2679\n",
      "train auc: 0.964011240692\n",
      "test auc: 0.965154989101\n",
      "loss at iter 361:0.2682\n",
      "train auc: 0.964063088053\n",
      "test auc: 0.965191264767\n",
      "loss at iter 362:0.2675\n",
      "train auc: 0.964062849297\n",
      "test auc: 0.965199456988\n",
      "loss at iter 363:0.2678\n",
      "train auc: 0.964113853458\n",
      "test auc: 0.965236804096\n",
      "loss at iter 364:0.2671\n",
      "train auc: 0.964113815637\n",
      "test auc: 0.965244310402\n",
      "loss at iter 365:0.2673\n",
      "train auc: 0.964164781186\n",
      "test auc: 0.965280954159\n",
      "loss at iter 366:0.2666\n",
      "train auc: 0.964164148622\n",
      "test auc: 0.965289037279\n",
      "loss at iter 367:0.2669\n",
      "train auc: 0.964215140541\n",
      "test auc: 0.965323990343\n",
      "loss at iter 368:0.2662\n",
      "train auc: 0.964214746448\n",
      "test auc: 0.965334558588\n",
      "loss at iter 369:0.2665\n",
      "train auc: 0.964265039299\n",
      "test auc: 0.965368768435\n",
      "loss at iter 370:0.2658\n",
      "train auc: 0.964264584066\n",
      "test auc: 0.965378688106\n",
      "loss at iter 371:0.2661\n",
      "train auc: 0.964315035932\n",
      "test auc: 0.965413161752\n",
      "loss at iter 372:0.2654\n",
      "train auc: 0.964313691406\n",
      "test auc: 0.965421325041\n",
      "loss at iter 373:0.2657\n",
      "train auc: 0.964364227984\n",
      "test auc: 0.965456265061\n",
      "loss at iter 374:0.2650\n",
      "train auc: 0.964363026212\n",
      "test auc: 0.965464713125\n",
      "loss at iter 375:0.2652\n",
      "train auc: 0.964413369764\n",
      "test auc: 0.965499821282\n",
      "loss at iter 376:0.2645\n",
      "train auc: 0.964411566311\n",
      "test auc: 0.965507504407\n",
      "loss at iter 377:0.2648\n",
      "train auc: 0.964461698058\n",
      "test auc: 0.965543018931\n",
      "loss at iter 378:0.2641\n",
      "train auc: 0.964459060858\n",
      "test auc: 0.965549369861\n",
      "loss at iter 379:0.2644\n",
      "train auc: 0.964509146653\n",
      "test auc: 0.965586876635\n",
      "loss at iter 380:0.2637\n",
      "train auc: 0.96450587524\n",
      "test auc: 0.965592223232\n",
      "loss at iter 381:0.2640\n",
      "train auc: 0.964555353399\n",
      "test auc: 0.965627307407\n",
      "loss at iter 382:0.2633\n",
      "train auc: 0.964551893529\n",
      "test auc: 0.965633371563\n",
      "loss at iter 383:0.2636\n",
      "train auc: 0.964600836437\n",
      "test auc: 0.965668372294\n",
      "loss at iter 384:0.2629\n",
      "train auc: 0.964597793635\n",
      "test auc: 0.965673343463\n",
      "loss at iter 385:0.2632\n",
      "train auc: 0.964646589691\n",
      "test auc: 0.965708328245\n",
      "loss at iter 386:0.2625\n",
      "train auc: 0.964643697396\n",
      "test auc: 0.96571246108\n",
      "loss at iter 387:0.2628\n",
      "train auc: 0.964691758871\n",
      "test auc: 0.965747148367\n",
      "loss at iter 388:0.2621\n",
      "train auc: 0.964688886593\n",
      "test auc: 0.965752740401\n",
      "loss at iter 389:0.2624\n",
      "train auc: 0.964737268841\n",
      "test auc: 0.96578703514\n",
      "loss at iter 390:0.2617\n",
      "train auc: 0.964734186631\n",
      "test auc: 0.965791983959\n",
      "loss at iter 391:0.2620\n",
      "train auc: 0.964782110045\n",
      "test auc: 0.965825426064\n",
      "loss at iter 392:0.2613\n",
      "train auc: 0.964779292977\n",
      "test auc: 0.965831522043\n",
      "loss at iter 393:0.2616\n",
      "train auc: 0.96482701563\n",
      "test auc: 0.965865657878\n",
      "loss at iter 394:0.2610\n",
      "train auc: 0.964824472506\n",
      "test auc: 0.965871453171\n",
      "loss at iter 395:0.2612\n",
      "train auc: 0.964871081961\n",
      "test auc: 0.96590524797\n",
      "loss at iter 396:0.2606\n",
      "train auc: 0.964868793467\n",
      "test auc: 0.965908277316\n",
      "loss at iter 397:0.2608\n",
      "train auc: 0.964915211677\n",
      "test auc: 0.965942266896\n",
      "loss at iter 398:0.2602\n",
      "train auc: 0.964912914472\n",
      "test auc: 0.96594643081\n",
      "loss at iter 399:0.2604\n",
      "train auc: 0.964958344611\n",
      "test auc: 0.965979806002\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "trainloss = list()\n",
    "testloss  = list()\n",
    "acctrain  = list()\n",
    "acctest   = list()\n",
    "s.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "# run optimizing iterations\n",
    "for i in range(400):\n",
    "    #batchX, batchY = s.run(tf.train.batch([X_train_flat, y_train_oh],100,enqueue_many=True, capacity=1))\n",
    "    s.run(optimizer, {input_X: X_train_flat, input_y: y_train_oh})\n",
    "    #s.run(optimizer, {input_X: batchX, input_y: batchY})\n",
    "    loss_i = s.run(loss, {input_X: X_train_flat, input_y: y_train_oh})\n",
    "    trainloss.append(loss_i)\n",
    "    loss_i = s.run(loss, {input_X: X_test_flat, input_y: y_test_oh})\n",
    "    testloss.append(loss_i)\n",
    "    acctrain.append(s.run(accuracy, feed_dict={input_X:X_train_flat, input_y: y_train_oh}))\n",
    "    acctest.append(s.run(accuracy, feed_dict={input_X:X_test_flat, input_y: y_test_oh}))\n",
    "    print(\"loss at iter %i:%.4f\" % (i, loss_i))\n",
    "    print(\"train auc:\", roc_auc_score(y_train_oh, s.run(predicted_y, {input_X:X_train_flat})))\n",
    "    print(\"test auc:\", roc_auc_score(y_test_oh, s.run(predicted_y, {input_X:X_test_flat})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEdCAYAAAAPT9w1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDeL/A1fFOn0BT1AvAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12a800be278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEdCAYAAADjFntmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQUREuigpiIhIFyUFERHpoqQgIiJd/j+JRJgFYhP7+wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12aa94a7940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(trainloss, label = \"loss train\")\n",
    "plt.plot(testloss, label=\"loss test\")\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('iteration')\n",
    "plt.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc=3,\n",
    "           ncol=2, mode=\"expand\", borderaxespad=0.)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(acctrain, label = \"accuracy train\")\n",
    "plt.plot(acctest, label  = \"accuracy test\")\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('iteration')\n",
    "plt.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc=3,\n",
    "           ncol=2, mode=\"expand\", borderaxespad=0.)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on train data: 0.92158\n",
      "Accuracy on val data: 0.9311\n",
      "Accuracy on test data: 0.9208\n"
     ]
    }
   ],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(predicted_y,1), tf.argmax(input_y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(\"Accuracy on train data:\", s.run(accuracy, feed_dict={input_X:X_train_flat, input_y: y_train_oh}))\n",
    "print(\"Accuracy on val data:\"  , s.run(accuracy, feed_dict={input_X:X_val_flat, input_y: y_val_oh}))\n",
    "print(\"Accuracy on test data:\" , s.run(accuracy, feed_dict={input_X:X_test_flat, input_y: y_test_oh}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
