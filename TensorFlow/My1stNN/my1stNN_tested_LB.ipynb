{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Simplest NN\n",
    "This code builds a 2-layer NN and applies it to recogition of handwritten digits.\n",
    "Tested on Deep Learning AMI Amazon Linux - 3.3_Oct2017 (ami-999844e0),\n",
    "Instance type m4.xlarge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we load 'mnist' dataset of handwritten images (https://keras.io/datasets/) with help of an already provided function *load_dataset*. Be sure that the file *preprocessed_mnist.py* is in your working directory.\n",
    "\n",
    "Now we also want to know the shapes of the original train/test-data and visualize some symbols from the train data. We will want to classify each handwritten symbol as a digit: 0,1,...,9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
      "10469376/11490434 [==========================>...] - ETA: 0s(50000, 28, 28) (50000,)\n",
      "(10000, 28, 28) (10000,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fc774ff86d8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nICjCDwRF+IGg/g9n4oRbuSFGWgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc7847b1c88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# os.chdir(\"D:\\\\Downloads\\\\\")\n",
    "from preprocessed_mnist import load_dataset\n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_dataset()\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.imshow(X_train[5], cmap=\"Greys\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want the image data to be in a flat format, so let's reload the reshaped version of the dataset and print new shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X and y train shape: (50000, 784) (50000,)\n",
      "X and y test shape: (10000, 784) (10000,)\n"
     ]
    }
   ],
   "source": [
    "X_train_flat, y_train, X_val_flat, y_val, X_test_flat, y_test = load_dataset(flatten=True)\n",
    "print(\"X and y train shape:\", X_train_flat.shape, y_train.shape)\n",
    "print(\"X and y test shape:\", X_test_flat.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also want to convert categorical labels of 10 classes to binary format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = tf.Session()\n",
    "\n",
    "# Categorical labels to binaries\n",
    "y_train_oh = s.run(tf.one_hot(y_train, 10))\n",
    "y_test_oh  = s.run(tf.one_hot(y_test, 10))\n",
    "y_val_oh   = s.run(tf.one_hot(y_val, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression\n",
    "Here we will implement our simple NN with 1 hidden layer having 50 neurons.\n",
    "\n",
    "First let's create variables for weights and biases and placeholders for input data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'Placeholder:0' shape=(?, 784) dtype=float32>,\n",
       " <tf.Tensor 'Placeholder_1:0' shape=(?, 10) dtype=float32>,\n",
       " <tf.Variable 'weights_h:0' shape=(784, 50) dtype=float32_ref>,\n",
       " <tf.Variable 'weights:0' shape=(50, 10) dtype=float32_ref>,\n",
       " <tf.Variable 'biases_h:0' shape=(50,) dtype=float32_ref>,\n",
       " <tf.Variable 'biases:0' shape=(10,) dtype=float32_ref>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model parameters - weights and bias\n",
    "nhid   = 50\n",
    "nclass = len(np.unique(y_train))\n",
    "weights_hid = tf.Variable(tf.random_normal([X_train_flat.shape[1], nhid], stddev=0.35),\n",
    "                      name=\"weights_h\") \n",
    "\n",
    "b_hid = tf.Variable(tf.zeros([nhid]), dtype='float32', name=\"biases_h\")\n",
    "\n",
    "weights_out = tf.Variable(tf.random_normal([nhid, nclass], stddev=0.35),\n",
    "                      name=\"weights\") \n",
    "\n",
    "b_out = tf.Variable(tf.zeros([nclass]), dtype='float32', name=\"biases\")\n",
    "\n",
    "# Placeholders for the input data\n",
    "input_X = tf.placeholder('float32', shape=(None,X_train_flat.shape[1]))\n",
    "input_y = tf.placeholder('float32', shape=(None, nclass))\n",
    "input_X, input_y, weights_hid, weights_out, b_hid, b_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now implement the model itself. The outpit layer uses softmax non-linearity, hidden layer - *relu* activation function.\n",
    "\n",
    "**Loss** will be a softmax cross entropy between log probabilities and labels.  \n",
    "**Optimization method** - gradient decent.  \n",
    "**Accuracy** is just a proportion of correct predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "#predicted_y =  tf.nn.softmax(tf.matmul(input_X, weights)+b)\n",
    "predicted_y_hid =  tf.nn.relu(tf.matmul(input_X, weights_hid)+b_hid)\n",
    "predicted_y     =  tf.matmul(predicted_y_hid, weights_out)+b_out\n",
    "\n",
    "# Loss. Should be a scalar number - average loss over all the objects\n",
    "#loss = tf.reduce_mean(-tf.reduce_sum(tf.log(predicted_y+1e-07)*input_y, reduction_indices=[1]))\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=input_y, logits=predicted_y))\n",
    "\n",
    "# See above for an example. tf.train.*Optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.5).minimize(\n",
    "    loss, var_list=(weights_hid, b_hid, weights_out, b_out))\n",
    "\n",
    "# compute accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(predicted_y,1), tf.argmax(input_y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will make 400 learning iterations and also show loss, auc on train and test data for each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at iter 0:6.4002\n",
      "train auc: 0.668213788996\n",
      "test auc: 0.673123772612\n",
      "loss at iter 1:4.3656\n",
      "train auc: 0.720813859766\n",
      "test auc: 0.729215355947\n",
      "loss at iter 2:2.0133\n",
      "train auc: 0.763423154025\n",
      "test auc: 0.770695971169\n",
      "loss at iter 3:1.6676\n",
      "train auc: 0.792972168506\n",
      "test auc: 0.800889618971\n",
      "loss at iter 4:1.5208\n",
      "train auc: 0.811522323523\n",
      "test auc: 0.819310601054\n",
      "loss at iter 5:1.4097\n",
      "train auc: 0.826611364539\n",
      "test auc: 0.834239930252\n",
      "loss at iter 6:1.3180\n",
      "train auc: 0.839471938465\n",
      "test auc: 0.846847623978\n",
      "loss at iter 7:1.2404\n",
      "train auc: 0.85058253059\n",
      "test auc: 0.8576339799\n",
      "loss at iter 8:1.1729\n",
      "train auc: 0.860292215288\n",
      "test auc: 0.867071367751\n",
      "loss at iter 9:1.1140\n",
      "train auc: 0.868812701412\n",
      "test auc: 0.875288257712\n",
      "loss at iter 10:1.0618\n",
      "train auc: 0.876323655252\n",
      "test auc: 0.882542302366\n",
      "loss at iter 11:1.0158\n",
      "train auc: 0.882910931532\n",
      "test auc: 0.888871278322\n",
      "loss at iter 12:0.9748\n",
      "train auc: 0.888727771482\n",
      "test auc: 0.89442004594\n",
      "loss at iter 13:0.9383\n",
      "train auc: 0.893878525634\n",
      "test auc: 0.899308462044\n",
      "loss at iter 14:0.9056\n",
      "train auc: 0.898449715068\n",
      "test auc: 0.903658480346\n",
      "loss at iter 15:0.8760\n",
      "train auc: 0.902511059773\n",
      "test auc: 0.907540045329\n",
      "loss at iter 16:0.8493\n",
      "train auc: 0.9061286311\n",
      "test auc: 0.910997108414\n",
      "loss at iter 17:0.8248\n",
      "train auc: 0.909375488334\n",
      "test auc: 0.914118768083\n",
      "loss at iter 18:0.8025\n",
      "train auc: 0.912306340004\n",
      "test auc: 0.916923723516\n",
      "loss at iter 19:0.7818\n",
      "train auc: 0.914975970609\n",
      "test auc: 0.919471372221\n",
      "loss at iter 20:0.7628\n",
      "train auc: 0.917405675374\n",
      "test auc: 0.921780258697\n",
      "loss at iter 21:0.7452\n",
      "train auc: 0.919616874201\n",
      "test auc: 0.923876209001\n",
      "loss at iter 22:0.7289\n",
      "train auc: 0.921641523914\n",
      "test auc: 0.925794128467\n",
      "loss at iter 23:0.7137\n",
      "train auc: 0.923507193878\n",
      "test auc: 0.927562609081\n",
      "loss at iter 24:0.6995\n",
      "train auc: 0.92522695\n",
      "test auc: 0.929183724793\n",
      "loss at iter 25:0.6862\n",
      "train auc: 0.926820121242\n",
      "test auc: 0.930700815473\n",
      "loss at iter 26:0.6737\n",
      "train auc: 0.928299016809\n",
      "test auc: 0.932099569958\n",
      "loss at iter 27:0.6620\n",
      "train auc: 0.929678042639\n",
      "test auc: 0.93338880961\n",
      "loss at iter 28:0.6510\n",
      "train auc: 0.930964317261\n",
      "test auc: 0.934597483747\n",
      "loss at iter 29:0.6407\n",
      "train auc: 0.932171525948\n",
      "test auc: 0.935715277571\n",
      "loss at iter 30:0.6309\n",
      "train auc: 0.933301635664\n",
      "test auc: 0.936773494011\n",
      "loss at iter 31:0.6217\n",
      "train auc: 0.934363196223\n",
      "test auc: 0.937754707913\n",
      "loss at iter 32:0.6130\n",
      "train auc: 0.935352625651\n",
      "test auc: 0.938682174984\n",
      "loss at iter 33:0.6047\n",
      "train auc: 0.936285466255\n",
      "test auc: 0.939541759385\n",
      "loss at iter 34:0.5968\n",
      "train auc: 0.937169682706\n",
      "test auc: 0.940364396937\n",
      "loss at iter 35:0.5893\n",
      "train auc: 0.938011130529\n",
      "test auc: 0.941143466058\n",
      "loss at iter 36:0.5821\n",
      "train auc: 0.938806798074\n",
      "test auc: 0.941876335344\n",
      "loss at iter 37:0.5753\n",
      "train auc: 0.939564001181\n",
      "test auc: 0.942570471122\n",
      "loss at iter 38:0.5688\n",
      "train auc: 0.94028317643\n",
      "test auc: 0.943229961316\n",
      "loss at iter 39:0.5625\n",
      "train auc: 0.940969794399\n",
      "test auc: 0.943857164965\n",
      "loss at iter 40:0.5565\n",
      "train auc: 0.941625480735\n",
      "test auc: 0.944458326452\n",
      "loss at iter 41:0.5507\n",
      "train auc: 0.94225043539\n",
      "test auc: 0.945041586156\n",
      "loss at iter 42:0.5452\n",
      "train auc: 0.942847118116\n",
      "test auc: 0.945594213274\n",
      "loss at iter 43:0.5399\n",
      "train auc: 0.943417029789\n",
      "test auc: 0.946120210658\n",
      "loss at iter 44:0.5348\n",
      "train auc: 0.943962110982\n",
      "test auc: 0.946619959581\n",
      "loss at iter 45:0.5298\n",
      "train auc: 0.944486267271\n",
      "test auc: 0.947104262277\n",
      "loss at iter 46:0.5251\n",
      "train auc: 0.944987093922\n",
      "test auc: 0.94756883483\n",
      "loss at iter 47:0.5205\n",
      "train auc: 0.945468475707\n",
      "test auc: 0.948015545418\n",
      "loss at iter 48:0.5161\n",
      "train auc: 0.945930251464\n",
      "test auc: 0.948441611482\n",
      "loss at iter 49:0.5118\n",
      "train auc: 0.94637548534\n",
      "test auc: 0.948854143038\n",
      "loss at iter 50:0.5077\n",
      "train auc: 0.946805598016\n",
      "test auc: 0.949251695685\n",
      "loss at iter 51:0.5037\n",
      "train auc: 0.947221841434\n",
      "test auc: 0.949637111296\n",
      "loss at iter 52:0.4998\n",
      "train auc: 0.947623549067\n",
      "test auc: 0.950009648201\n",
      "loss at iter 53:0.4961\n",
      "train auc: 0.948010497311\n",
      "test auc: 0.950361873132\n",
      "loss at iter 54:0.4925\n",
      "train auc: 0.948384696295\n",
      "test auc: 0.950708132615\n",
      "loss at iter 55:0.4889\n",
      "train auc: 0.948746396355\n",
      "test auc: 0.951041119434\n",
      "loss at iter 56:0.4855\n",
      "train auc: 0.94909687031\n",
      "test auc: 0.951362447557\n",
      "loss at iter 57:0.4822\n",
      "train auc: 0.949437234991\n",
      "test auc: 0.951677373345\n",
      "loss at iter 58:0.4790\n",
      "train auc: 0.949766229796\n",
      "test auc: 0.951976612962\n",
      "loss at iter 59:0.4759\n",
      "train auc: 0.950084308234\n",
      "test auc: 0.952273160094\n",
      "loss at iter 60:0.4729\n",
      "train auc: 0.950393203949\n",
      "test auc: 0.952556055338\n",
      "loss at iter 61:0.4699\n",
      "train auc: 0.950693665858\n",
      "test auc: 0.952830024679\n",
      "loss at iter 62:0.4670\n",
      "train auc: 0.950986520916\n",
      "test auc: 0.953099178883\n",
      "loss at iter 63:0.4642\n",
      "train auc: 0.951269732083\n",
      "test auc: 0.953360742022\n",
      "loss at iter 64:0.4615\n",
      "train auc: 0.951545004344\n",
      "test auc: 0.953612275992\n",
      "loss at iter 65:0.4589\n",
      "train auc: 0.951812711398\n",
      "test auc: 0.953859431779\n",
      "loss at iter 66:0.4563\n",
      "train auc: 0.952074358927\n",
      "test auc: 0.954095443269\n",
      "loss at iter 67:0.4538\n",
      "train auc: 0.952330360592\n",
      "test auc: 0.954327353219\n",
      "loss at iter 68:0.4513\n",
      "train auc: 0.952578504786\n",
      "test auc: 0.954553883474\n",
      "loss at iter 69:0.4489\n",
      "train auc: 0.952820642704\n",
      "test auc: 0.954773691622\n",
      "loss at iter 70:0.4465\n",
      "train auc: 0.953057446041\n",
      "test auc: 0.954989938409\n",
      "loss at iter 71:0.4442\n",
      "train auc: 0.953287850279\n",
      "test auc: 0.955199396123\n",
      "loss at iter 72:0.4419\n",
      "train auc: 0.95351311658\n",
      "test auc: 0.955402257879\n",
      "loss at iter 73:0.4397\n",
      "train auc: 0.953732599348\n",
      "test auc: 0.955601438167\n",
      "loss at iter 74:0.4376\n",
      "train auc: 0.953947434822\n",
      "test auc: 0.955798926133\n",
      "loss at iter 75:0.4355\n",
      "train auc: 0.954156697589\n",
      "test auc: 0.955990914043\n",
      "loss at iter 76:0.4334\n",
      "train auc: 0.954362411424\n",
      "test auc: 0.956176382356\n",
      "loss at iter 77:0.4314\n",
      "train auc: 0.954563509313\n",
      "test auc: 0.956360523211\n",
      "loss at iter 78:0.4294\n",
      "train auc: 0.954760507873\n",
      "test auc: 0.956538752976\n",
      "loss at iter 79:0.4275\n",
      "train auc: 0.954953284063\n",
      "test auc: 0.956716006354\n",
      "loss at iter 80:0.4256\n",
      "train auc: 0.955141955759\n",
      "test auc: 0.956886471583\n",
      "loss at iter 81:0.4238\n",
      "train auc: 0.955326826492\n",
      "test auc: 0.957057234004\n",
      "loss at iter 82:0.4219\n",
      "train auc: 0.955507377401\n",
      "test auc: 0.957221674319\n",
      "loss at iter 83:0.4201\n",
      "train auc: 0.955683907785\n",
      "test auc: 0.957384459106\n",
      "loss at iter 84:0.4184\n",
      "train auc: 0.955857572854\n",
      "test auc: 0.957542878312\n",
      "loss at iter 85:0.4166\n",
      "train auc: 0.956027040391\n",
      "test auc: 0.957697869595\n",
      "loss at iter 86:0.4149\n",
      "train auc: 0.956193545238\n",
      "test auc: 0.957850272394\n",
      "loss at iter 87:0.4133\n",
      "train auc: 0.956356889613\n",
      "test auc: 0.957997345674\n",
      "loss at iter 88:0.4117\n",
      "train auc: 0.956516970054\n",
      "test auc: 0.958142021994\n",
      "loss at iter 89:0.4100\n",
      "train auc: 0.956674836083\n",
      "test auc: 0.958286343814\n",
      "loss at iter 90:0.4085\n",
      "train auc: 0.956828242435\n",
      "test auc: 0.958424165542\n",
      "loss at iter 91:0.4069\n",
      "train auc: 0.956979972714\n",
      "test auc: 0.958560663872\n",
      "loss at iter 92:0.4054\n",
      "train auc: 0.957128571093\n",
      "test auc: 0.958696793116\n",
      "loss at iter 93:0.4039\n",
      "train auc: 0.957274502517\n",
      "test auc: 0.958828730709\n",
      "loss at iter 94:0.4024\n",
      "train auc: 0.957418568249\n",
      "test auc: 0.95896062497\n",
      "loss at iter 95:0.4010\n",
      "train auc: 0.957560039051\n",
      "test auc: 0.959089300132\n",
      "loss at iter 96:0.3995\n",
      "train auc: 0.957698729625\n",
      "test auc: 0.959213842832\n",
      "loss at iter 97:0.3981\n",
      "train auc: 0.957835567695\n",
      "test auc: 0.959338498387\n",
      "loss at iter 98:0.3967\n",
      "train auc: 0.957970184929\n",
      "test auc: 0.959463131206\n",
      "loss at iter 99:0.3954\n",
      "train auc: 0.958103358306\n",
      "test auc: 0.959583168551\n",
      "loss at iter 100:0.3940\n",
      "train auc: 0.958234728355\n",
      "test auc: 0.959702579633\n",
      "loss at iter 101:0.3927\n",
      "train auc: 0.958364543579\n",
      "test auc: 0.959820211906\n",
      "loss at iter 102:0.3914\n",
      "train auc: 0.958491210718\n",
      "test auc: 0.959937322908\n",
      "loss at iter 103:0.3901\n",
      "train auc: 0.958616445888\n",
      "test auc: 0.960050531949\n",
      "loss at iter 104:0.3888\n",
      "train auc: 0.958739711891\n",
      "test auc: 0.960162701826\n",
      "loss at iter 105:0.3875\n",
      "train auc: 0.95886164843\n",
      "test auc: 0.960272988528\n",
      "loss at iter 106:0.3863\n",
      "train auc: 0.958982010672\n",
      "test auc: 0.960382534911\n",
      "loss at iter 107:0.3851\n",
      "train auc: 0.959100668456\n",
      "test auc: 0.960493039127\n",
      "loss at iter 108:0.3839\n",
      "train auc: 0.959217484049\n",
      "test auc: 0.960597890241\n",
      "loss at iter 109:0.3827\n",
      "train auc: 0.95933350011\n",
      "test auc: 0.960700218929\n",
      "loss at iter 110:0.3815\n",
      "train auc: 0.959447252635\n",
      "test auc: 0.960804849283\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at iter 111:0.3803\n",
      "train auc: 0.959559930083\n",
      "test auc: 0.960906395956\n",
      "loss at iter 112:0.3792\n",
      "train auc: 0.959670389448\n",
      "test auc: 0.96100583861\n",
      "loss at iter 113:0.3780\n",
      "train auc: 0.959779909056\n",
      "test auc: 0.961105510327\n",
      "loss at iter 114:0.3769\n",
      "train auc: 0.959887122646\n",
      "test auc: 0.961201823319\n",
      "loss at iter 115:0.3758\n",
      "train auc: 0.959994021401\n",
      "test auc: 0.961297574967\n",
      "loss at iter 116:0.3747\n",
      "train auc: 0.960099316462\n",
      "test auc: 0.961394584493\n",
      "loss at iter 117:0.3736\n",
      "train auc: 0.960203557327\n",
      "test auc: 0.961486832685\n",
      "loss at iter 118:0.3726\n",
      "train auc: 0.960306589796\n",
      "test auc: 0.961582302239\n",
      "loss at iter 119:0.3715\n",
      "train auc: 0.960407814363\n",
      "test auc: 0.961674111554\n",
      "loss at iter 120:0.3705\n",
      "train auc: 0.960508028061\n",
      "test auc: 0.961762291933\n",
      "loss at iter 121:0.3694\n",
      "train auc: 0.960607403026\n",
      "test auc: 0.961852882511\n",
      "loss at iter 122:0.3684\n",
      "train auc: 0.960705640648\n",
      "test auc: 0.961940751365\n",
      "loss at iter 123:0.3674\n",
      "train auc: 0.960801858275\n",
      "test auc: 0.962028291813\n",
      "loss at iter 124:0.3664\n",
      "train auc: 0.96089783205\n",
      "test auc: 0.962114197011\n",
      "loss at iter 125:0.3654\n",
      "train auc: 0.960992036582\n",
      "test auc: 0.962200040784\n",
      "loss at iter 126:0.3644\n",
      "train auc: 0.961084599759\n",
      "test auc: 0.962283982575\n",
      "loss at iter 127:0.3635\n",
      "train auc: 0.96117620474\n",
      "test auc: 0.962365489179\n",
      "loss at iter 128:0.3625\n",
      "train auc: 0.961267035802\n",
      "test auc: 0.962445951916\n",
      "loss at iter 129:0.3616\n",
      "train auc: 0.96135724883\n",
      "test auc: 0.962527073112\n",
      "loss at iter 130:0.3607\n",
      "train auc: 0.961446726556\n",
      "test auc: 0.962606989643\n",
      "loss at iter 131:0.3597\n",
      "train auc: 0.961534744409\n",
      "test auc: 0.962688622745\n",
      "loss at iter 132:0.3588\n",
      "train auc: 0.961621514142\n",
      "test auc: 0.962767033991\n",
      "loss at iter 133:0.3579\n",
      "train auc: 0.961707851374\n",
      "test auc: 0.962846480877\n",
      "loss at iter 134:0.3570\n",
      "train auc: 0.961793628946\n",
      "test auc: 0.962924339254\n",
      "loss at iter 135:0.3561\n",
      "train auc: 0.961878875752\n",
      "test auc: 0.963001095719\n",
      "loss at iter 136:0.3553\n",
      "train auc: 0.96196383888\n",
      "test auc: 0.963076169596\n",
      "loss at iter 137:0.3544\n",
      "train auc: 0.962047163389\n",
      "test auc: 0.96315217065\n",
      "loss at iter 138:0.3535\n",
      "train auc: 0.962129997209\n",
      "test auc: 0.963226159924\n",
      "loss at iter 139:0.3527\n",
      "train auc: 0.962212169478\n",
      "test auc: 0.963299090004\n",
      "loss at iter 140:0.3518\n",
      "train auc: 0.962293178712\n",
      "test auc: 0.963370989608\n",
      "loss at iter 141:0.3510\n",
      "train auc: 0.962373829954\n",
      "test auc: 0.96344160917\n",
      "loss at iter 142:0.3502\n",
      "train auc: 0.96245382042\n",
      "test auc: 0.963511801697\n",
      "loss at iter 143:0.3494\n",
      "train auc: 0.962533085082\n",
      "test auc: 0.963583181098\n",
      "loss at iter 144:0.3485\n",
      "train auc: 0.962612024872\n",
      "test auc: 0.963651660521\n",
      "loss at iter 145:0.3477\n",
      "train auc: 0.962690284018\n",
      "test auc: 0.963723026579\n",
      "loss at iter 146:0.3469\n",
      "train auc: 0.962767258479\n",
      "test auc: 0.96379228643\n",
      "loss at iter 147:0.3462\n",
      "train auc: 0.962843821333\n",
      "test auc: 0.963861204906\n",
      "loss at iter 148:0.3454\n",
      "train auc: 0.962919568305\n",
      "test auc: 0.963928939089\n",
      "loss at iter 149:0.3446\n",
      "train auc: 0.962994823256\n",
      "test auc: 0.963993357896\n",
      "loss at iter 150:0.3438\n",
      "train auc: 0.963070129551\n",
      "test auc: 0.964059549748\n",
      "loss at iter 151:0.3431\n",
      "train auc: 0.963144239741\n",
      "test auc: 0.964125515757\n",
      "loss at iter 152:0.3423\n",
      "train auc: 0.963218536548\n",
      "test auc: 0.964190715523\n",
      "loss at iter 153:0.3416\n",
      "train auc: 0.963292279688\n",
      "test auc: 0.964255555459\n",
      "loss at iter 154:0.3408\n",
      "train auc: 0.963365408141\n",
      "test auc: 0.964320891838\n",
      "loss at iter 155:0.3401\n",
      "train auc: 0.963437834377\n",
      "test auc: 0.964384287569\n",
      "loss at iter 156:0.3394\n",
      "train auc: 0.963510137654\n",
      "test auc: 0.964449161583\n",
      "loss at iter 157:0.3386\n",
      "train auc: 0.963581760434\n",
      "test auc: 0.964510944987\n",
      "loss at iter 158:0.3379\n",
      "train auc: 0.963652592984\n",
      "test auc: 0.96457519555\n",
      "loss at iter 159:0.3372\n",
      "train auc: 0.96372318862\n",
      "test auc: 0.964637457561\n",
      "loss at iter 160:0.3365\n",
      "train auc: 0.963793456533\n",
      "test auc: 0.96469829723\n",
      "loss at iter 161:0.3358\n",
      "train auc: 0.963862509605\n",
      "test auc: 0.964759866346\n",
      "loss at iter 162:0.3351\n",
      "train auc: 0.963931261819\n",
      "test auc: 0.964820869691\n",
      "loss at iter 163:0.3344\n",
      "train auc: 0.963999078363\n",
      "test auc: 0.964879211568\n",
      "loss at iter 164:0.3338\n",
      "train auc: 0.964066916271\n",
      "test auc: 0.964937037856\n",
      "loss at iter 165:0.3331\n",
      "train auc: 0.964133874269\n",
      "test auc: 0.964994059758\n",
      "loss at iter 166:0.3324\n",
      "train auc: 0.964200597284\n",
      "test auc: 0.965051690404\n",
      "loss at iter 167:0.3318\n",
      "train auc: 0.964266370292\n",
      "test auc: 0.96510890202\n",
      "loss at iter 168:0.3311\n",
      "train auc: 0.964332343177\n",
      "test auc: 0.965166896586\n",
      "loss at iter 169:0.3304\n",
      "train auc: 0.964397126168\n",
      "test auc: 0.965224116576\n",
      "loss at iter 170:0.3298\n",
      "train auc: 0.964461582294\n",
      "test auc: 0.965280562036\n",
      "loss at iter 171:0.3291\n",
      "train auc: 0.964525585499\n",
      "test auc: 0.965336401099\n",
      "loss at iter 172:0.3285\n",
      "train auc: 0.964589340598\n",
      "test auc: 0.965394061085\n",
      "loss at iter 173:0.3279\n",
      "train auc: 0.964652797176\n",
      "test auc: 0.965450362622\n",
      "loss at iter 174:0.3272\n",
      "train auc: 0.964716228465\n",
      "test auc: 0.965506668208\n",
      "loss at iter 175:0.3266\n",
      "train auc: 0.96477852812\n",
      "test auc: 0.96556112905\n",
      "loss at iter 176:0.3260\n",
      "train auc: 0.964840983009\n",
      "test auc: 0.965618064263\n",
      "loss at iter 177:0.3254\n",
      "train auc: 0.964903097837\n",
      "test auc: 0.965673929557\n",
      "loss at iter 178:0.3248\n",
      "train auc: 0.964965357166\n",
      "test auc: 0.965728607536\n",
      "loss at iter 179:0.3241\n",
      "train auc: 0.965026657401\n",
      "test auc: 0.965783078387\n",
      "loss at iter 180:0.3235\n",
      "train auc: 0.96508755825\n",
      "test auc: 0.965837990532\n",
      "loss at iter 181:0.3229\n",
      "train auc: 0.965148018543\n",
      "test auc: 0.965890353323\n",
      "loss at iter 182:0.3224\n",
      "train auc: 0.965208573713\n",
      "test auc: 0.965945229586\n",
      "loss at iter 183:0.3218\n",
      "train auc: 0.965268542668\n",
      "test auc: 0.965997991763\n",
      "loss at iter 184:0.3212\n",
      "train auc: 0.965327952577\n",
      "test auc: 0.966047597224\n",
      "loss at iter 185:0.3206\n",
      "train auc: 0.965386230952\n",
      "test auc: 0.966099050831\n",
      "loss at iter 186:0.3200\n",
      "train auc: 0.965444268295\n",
      "test auc: 0.966149947655\n",
      "loss at iter 187:0.3194\n",
      "train auc: 0.965501514025\n",
      "test auc: 0.966200930908\n",
      "loss at iter 188:0.3189\n",
      "train auc: 0.965558771493\n",
      "test auc: 0.966251534682\n",
      "loss at iter 189:0.3183\n",
      "train auc: 0.965615558945\n",
      "test auc: 0.966300386173\n",
      "loss at iter 190:0.3177\n",
      "train auc: 0.965672770458\n",
      "test auc: 0.966349339183\n",
      "loss at iter 191:0.3172\n",
      "train auc: 0.965728851736\n",
      "test auc: 0.966400018868\n",
      "loss at iter 192:0.3166\n",
      "train auc: 0.965785664055\n",
      "test auc: 0.966449957829\n",
      "loss at iter 193:0.3161\n",
      "train auc: 0.965842079234\n",
      "test auc: 0.966500754932\n",
      "loss at iter 194:0.3155\n",
      "train auc: 0.965898104497\n",
      "test auc: 0.966550609535\n",
      "loss at iter 195:0.3150\n",
      "train auc: 0.965953463915\n",
      "test auc: 0.966598345477\n",
      "loss at iter 196:0.3144\n",
      "train auc: 0.966008834741\n",
      "test auc: 0.966646224667\n",
      "loss at iter 197:0.3139\n",
      "train auc: 0.96606398369\n",
      "test auc: 0.9666930252\n",
      "loss at iter 198:0.3134\n",
      "train auc: 0.966119095445\n",
      "test auc: 0.966741085171\n",
      "loss at iter 199:0.3129\n",
      "train auc: 0.966173247827\n",
      "test auc: 0.966789472184\n",
      "loss at iter 200:0.3123\n",
      "train auc: 0.966227522419\n",
      "test auc: 0.96683673457\n",
      "loss at iter 201:0.3118\n",
      "train auc: 0.966281051589\n",
      "test auc: 0.966883534224\n",
      "loss at iter 202:0.3113\n",
      "train auc: 0.966334992721\n",
      "test auc: 0.96693042139\n",
      "loss at iter 203:0.3108\n",
      "train auc: 0.966387978619\n",
      "test auc: 0.966976453203\n",
      "loss at iter 204:0.3103\n",
      "train auc: 0.966440769691\n",
      "test auc: 0.967021840167\n",
      "loss at iter 205:0.3098\n",
      "train auc: 0.966492628249\n",
      "test auc: 0.967066745946\n",
      "loss at iter 206:0.3093\n",
      "train auc: 0.966545150172\n",
      "test auc: 0.96711237374\n",
      "loss at iter 207:0.3088\n",
      "train auc: 0.966596911331\n",
      "test auc: 0.967157599925\n",
      "loss at iter 208:0.3083\n",
      "train auc: 0.966648398295\n",
      "test auc: 0.967200743149\n",
      "loss at iter 209:0.3078\n",
      "train auc: 0.96669936287\n",
      "test auc: 0.967243864616\n",
      "loss at iter 210:0.3073\n",
      "train auc: 0.966750007576\n",
      "test auc: 0.96728873463\n",
      "loss at iter 211:0.3068\n",
      "train auc: 0.96680019541\n",
      "test auc: 0.967333762299\n",
      "loss at iter 212:0.3063\n",
      "train auc: 0.966850352638\n",
      "test auc: 0.967379176781\n",
      "loss at iter 213:0.3059\n",
      "train auc: 0.966899682045\n",
      "test auc: 0.9674225397\n",
      "loss at iter 214:0.3054\n",
      "train auc: 0.966948704959\n",
      "test auc: 0.96746664496\n",
      "loss at iter 215:0.3049\n",
      "train auc: 0.966997103392\n",
      "test auc: 0.967508810191\n",
      "loss at iter 216:0.3045\n",
      "train auc: 0.967045520519\n",
      "test auc: 0.967551148778\n",
      "loss at iter 217:0.3040\n",
      "train auc: 0.967093612545\n",
      "test auc: 0.967591947332\n",
      "loss at iter 218:0.3035\n",
      "train auc: 0.967141413506\n",
      "test auc: 0.967632758899\n",
      "loss at iter 219:0.3031\n",
      "train auc: 0.967188507218\n",
      "test auc: 0.967673668886\n",
      "loss at iter 220:0.3026\n",
      "train auc: 0.967235894662\n",
      "test auc: 0.967713473998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at iter 221:0.3022\n",
      "train auc: 0.967282385966\n",
      "test auc: 0.967753364408\n",
      "loss at iter 222:0.3017\n",
      "train auc: 0.967328776798\n",
      "test auc: 0.96779211986\n",
      "loss at iter 223:0.3013\n",
      "train auc: 0.967374382292\n",
      "test auc: 0.967831211292\n",
      "loss at iter 224:0.3008\n",
      "train auc: 0.967420035983\n",
      "test auc: 0.967870803496\n",
      "loss at iter 225:0.3004\n",
      "train auc: 0.967465184037\n",
      "test auc: 0.967909535415\n",
      "loss at iter 226:0.2999\n",
      "train auc: 0.967509879298\n",
      "test auc: 0.967947529129\n",
      "loss at iter 227:0.2995\n",
      "train auc: 0.967554505627\n",
      "test auc: 0.967984938852\n",
      "loss at iter 228:0.2991\n",
      "train auc: 0.967598725369\n",
      "test auc: 0.968022577414\n",
      "loss at iter 229:0.2987\n",
      "train auc: 0.967643371566\n",
      "test auc: 0.968062529994\n",
      "loss at iter 230:0.2982\n",
      "train auc: 0.967687408025\n",
      "test auc: 0.968099708105\n",
      "loss at iter 231:0.2978\n",
      "train auc: 0.967731370246\n",
      "test auc: 0.968135883735\n",
      "loss at iter 232:0.2974\n",
      "train auc: 0.96777512619\n",
      "test auc: 0.968172536885\n",
      "loss at iter 233:0.2970\n",
      "train auc: 0.967818651937\n",
      "test auc: 0.968209665494\n",
      "loss at iter 234:0.2966\n",
      "train auc: 0.967861998856\n",
      "test auc: 0.96824513556\n",
      "loss at iter 235:0.2962\n",
      "train auc: 0.967905515508\n",
      "test auc: 0.968280948724\n",
      "loss at iter 236:0.2957\n",
      "train auc: 0.967948826588\n",
      "test auc: 0.968315943095\n",
      "loss at iter 237:0.2953\n",
      "train auc: 0.967991696077\n",
      "test auc: 0.968352938351\n",
      "loss at iter 238:0.2949\n",
      "train auc: 0.968034361295\n",
      "test auc: 0.968389876506\n",
      "loss at iter 239:0.2945\n",
      "train auc: 0.968076791411\n",
      "test auc: 0.968425137217\n",
      "loss at iter 240:0.2941\n",
      "train auc: 0.968119579217\n",
      "test auc: 0.968460992975\n",
      "loss at iter 241:0.2937\n",
      "train auc: 0.968161679953\n",
      "test auc: 0.968496196922\n",
      "loss at iter 242:0.2933\n",
      "train auc: 0.968204248273\n",
      "test auc: 0.968531734036\n",
      "loss at iter 243:0.2929\n",
      "train auc: 0.968246619932\n",
      "test auc: 0.96856806532\n",
      "loss at iter 244:0.2925\n",
      "train auc: 0.968288159522\n",
      "test auc: 0.968602259252\n",
      "loss at iter 245:0.2921\n",
      "train auc: 0.968329792891\n",
      "test auc: 0.968638074896\n",
      "loss at iter 246:0.2918\n",
      "train auc: 0.968370764517\n",
      "test auc: 0.968672199012\n",
      "loss at iter 247:0.2914\n",
      "train auc: 0.96841180196\n",
      "test auc: 0.968708573326\n",
      "loss at iter 248:0.2910\n",
      "train auc: 0.968452539251\n",
      "test auc: 0.968741985664\n",
      "loss at iter 249:0.2906\n",
      "train auc: 0.968493179896\n",
      "test auc: 0.968776402565\n",
      "loss at iter 250:0.2902\n",
      "train auc: 0.968533661543\n",
      "test auc: 0.968809255923\n",
      "loss at iter 251:0.2898\n",
      "train auc: 0.968573824518\n",
      "test auc: 0.968841844438\n",
      "loss at iter 252:0.2894\n",
      "train auc: 0.968613830072\n",
      "test auc: 0.96887508845\n",
      "loss at iter 253:0.2891\n",
      "train auc: 0.968653247203\n",
      "test auc: 0.968909385923\n",
      "loss at iter 254:0.2887\n",
      "train auc: 0.96869252567\n",
      "test auc: 0.968942666663\n",
      "loss at iter 255:0.2883\n",
      "train auc: 0.96873112878\n",
      "test auc: 0.968974794907\n",
      "loss at iter 256:0.2880\n",
      "train auc: 0.968770083814\n",
      "test auc: 0.969006956527\n",
      "loss at iter 257:0.2876\n",
      "train auc: 0.968809060654\n",
      "test auc: 0.96903944814\n",
      "loss at iter 258:0.2872\n",
      "train auc: 0.968847612246\n",
      "test auc: 0.969072684856\n",
      "loss at iter 259:0.2869\n",
      "train auc: 0.968885838312\n",
      "test auc: 0.969104368461\n",
      "loss at iter 260:0.2865\n",
      "train auc: 0.968923124911\n",
      "test auc: 0.969134254275\n",
      "loss at iter 261:0.2861\n",
      "train auc: 0.968960424095\n",
      "test auc: 0.969165642152\n",
      "loss at iter 262:0.2858\n",
      "train auc: 0.968997400464\n",
      "test auc: 0.969196010138\n",
      "loss at iter 263:0.2854\n",
      "train auc: 0.969034261535\n",
      "test auc: 0.969227657022\n",
      "loss at iter 264:0.2851\n",
      "train auc: 0.969071211853\n",
      "test auc: 0.969257651703\n",
      "loss at iter 265:0.2847\n",
      "train auc: 0.969107663421\n",
      "test auc: 0.969287037806\n",
      "loss at iter 266:0.2844\n",
      "train auc: 0.969144335653\n",
      "test auc: 0.969316591791\n",
      "loss at iter 267:0.2840\n",
      "train auc: 0.969180986447\n",
      "test auc: 0.969345767452\n",
      "loss at iter 268:0.2837\n",
      "train auc: 0.969217656474\n",
      "test auc: 0.969377040838\n",
      "loss at iter 269:0.2833\n",
      "train auc: 0.969253703366\n",
      "test auc: 0.969405742566\n",
      "loss at iter 270:0.2830\n",
      "train auc: 0.969289781074\n",
      "test auc: 0.969435229897\n",
      "loss at iter 271:0.2826\n",
      "train auc: 0.969325933908\n",
      "test auc: 0.969464148375\n",
      "loss at iter 272:0.2823\n",
      "train auc: 0.969361542645\n",
      "test auc: 0.96949348352\n",
      "loss at iter 273:0.2820\n",
      "train auc: 0.969397593947\n",
      "test auc: 0.969522613871\n",
      "loss at iter 274:0.2816\n",
      "train auc: 0.969433662545\n",
      "test auc: 0.969551491953\n",
      "loss at iter 275:0.2813\n",
      "train auc: 0.969469663186\n",
      "test auc: 0.96958093793\n",
      "loss at iter 276:0.2810\n",
      "train auc: 0.969505266342\n",
      "test auc: 0.969609999385\n",
      "loss at iter 277:0.2806\n",
      "train auc: 0.969540694403\n",
      "test auc: 0.969637163246\n",
      "loss at iter 278:0.2803\n",
      "train auc: 0.969576007639\n",
      "test auc: 0.96966627737\n",
      "loss at iter 279:0.2800\n",
      "train auc: 0.969611331863\n",
      "test auc: 0.969694889247\n",
      "loss at iter 280:0.2796\n",
      "train auc: 0.969646478742\n",
      "test auc: 0.969724801576\n",
      "loss at iter 281:0.2793\n",
      "train auc: 0.969681531313\n",
      "test auc: 0.969753005069\n",
      "loss at iter 282:0.2790\n",
      "train auc: 0.969716517828\n",
      "test auc: 0.969781436501\n",
      "loss at iter 283:0.2787\n",
      "train auc: 0.96975100465\n",
      "test auc: 0.969809543544\n",
      "loss at iter 284:0.2783\n",
      "train auc: 0.969785353901\n",
      "test auc: 0.969837182863\n",
      "loss at iter 285:0.2780\n",
      "train auc: 0.969819486921\n",
      "test auc: 0.969867221754\n",
      "loss at iter 286:0.2777\n",
      "train auc: 0.969852937175\n",
      "test auc: 0.969895542029\n",
      "loss at iter 287:0.2774\n",
      "train auc: 0.969886124157\n",
      "test auc: 0.969923486324\n",
      "loss at iter 288:0.2771\n",
      "train auc: 0.969919408281\n",
      "test auc: 0.969950413355\n",
      "loss at iter 289:0.2767\n",
      "train auc: 0.969952437666\n",
      "test auc: 0.969977615041\n",
      "loss at iter 290:0.2764\n",
      "train auc: 0.969985230602\n",
      "test auc: 0.970003870815\n",
      "loss at iter 291:0.2761\n",
      "train auc: 0.970018295936\n",
      "test auc: 0.970029165996\n",
      "loss at iter 292:0.2758\n",
      "train auc: 0.970051526621\n",
      "test auc: 0.970057143985\n",
      "loss at iter 293:0.2755\n",
      "train auc: 0.9700848113\n",
      "test auc: 0.970083828476\n",
      "loss at iter 294:0.2752\n",
      "train auc: 0.97011755238\n",
      "test auc: 0.970113094696\n",
      "loss at iter 295:0.2749\n",
      "train auc: 0.970150449855\n",
      "test auc: 0.970140299572\n",
      "loss at iter 296:0.2746\n",
      "train auc: 0.970183265335\n",
      "test auc: 0.970166513288\n",
      "loss at iter 297:0.2743\n",
      "train auc: 0.970216440876\n",
      "test auc: 0.970193947495\n",
      "loss at iter 298:0.2740\n",
      "train auc: 0.970249541618\n",
      "test auc: 0.970221905903\n",
      "loss at iter 299:0.2737\n",
      "train auc: 0.970282700227\n",
      "test auc: 0.970249876581\n",
      "loss at iter 300:0.2734\n",
      "train auc: 0.970315679461\n",
      "test auc: 0.970277364668\n",
      "loss at iter 301:0.2731\n",
      "train auc: 0.97034818317\n",
      "test auc: 0.970304493317\n",
      "loss at iter 302:0.2728\n",
      "train auc: 0.970380653459\n",
      "test auc: 0.970330496667\n",
      "loss at iter 303:0.2725\n",
      "train auc: 0.970413632194\n",
      "test auc: 0.970358321043\n",
      "loss at iter 304:0.2722\n",
      "train auc: 0.970446327003\n",
      "test auc: 0.970384965955\n",
      "loss at iter 305:0.2719\n",
      "train auc: 0.970479177391\n",
      "test auc: 0.970413392204\n",
      "loss at iter 306:0.2716\n",
      "train auc: 0.970512744934\n",
      "test auc: 0.970440932091\n",
      "loss at iter 307:0.2713\n",
      "train auc: 0.9705457483\n",
      "test auc: 0.970467346155\n",
      "loss at iter 308:0.2710\n",
      "train auc: 0.970578049632\n",
      "test auc: 0.970492848709\n",
      "loss at iter 309:0.2707\n",
      "train auc: 0.970610179734\n",
      "test auc: 0.970521474685\n",
      "loss at iter 310:0.2704\n",
      "train auc: 0.970642145524\n",
      "test auc: 0.970548245274\n",
      "loss at iter 311:0.2701\n",
      "train auc: 0.970674460696\n",
      "test auc: 0.970574850168\n",
      "loss at iter 312:0.2698\n",
      "train auc: 0.970706462331\n",
      "test auc: 0.970602910927\n",
      "loss at iter 313:0.2695\n",
      "train auc: 0.970738324886\n",
      "test auc: 0.970628494996\n",
      "loss at iter 314:0.2693\n",
      "train auc: 0.970769889757\n",
      "test auc: 0.970653731668\n",
      "loss at iter 315:0.2690\n",
      "train auc: 0.970800993125\n",
      "test auc: 0.97067998185\n",
      "loss at iter 316:0.2687\n",
      "train auc: 0.970832034434\n",
      "test auc: 0.970705404063\n",
      "loss at iter 317:0.2684\n",
      "train auc: 0.970862942304\n",
      "test auc: 0.970731304894\n",
      "loss at iter 318:0.2681\n",
      "train auc: 0.970894176378\n",
      "test auc: 0.970756960609\n",
      "loss at iter 319:0.2679\n",
      "train auc: 0.970925207777\n",
      "test auc: 0.97078226757\n",
      "loss at iter 320:0.2676\n",
      "train auc: 0.970956101353\n",
      "test auc: 0.97080802394\n",
      "loss at iter 321:0.2673\n",
      "train auc: 0.970987164054\n",
      "test auc: 0.97083353995\n",
      "loss at iter 322:0.2670\n",
      "train auc: 0.971018072023\n",
      "test auc: 0.970858837391\n",
      "loss at iter 323:0.2668\n",
      "train auc: 0.971048771519\n",
      "test auc: 0.97088325723\n",
      "loss at iter 324:0.2665\n",
      "train auc: 0.971079388347\n",
      "test auc: 0.970909412378\n",
      "loss at iter 325:0.2662\n",
      "train auc: 0.971109875805\n",
      "test auc: 0.97093486025\n",
      "loss at iter 326:0.2659\n",
      "train auc: 0.971139808323\n",
      "test auc: 0.970960164404\n",
      "loss at iter 327:0.2657\n",
      "train auc: 0.971169747848\n",
      "test auc: 0.970985385462\n",
      "loss at iter 328:0.2654\n",
      "train auc: 0.971199204966\n",
      "test auc: 0.971009500297\n",
      "loss at iter 329:0.2651\n",
      "train auc: 0.971228489344\n",
      "test auc: 0.971034564203\n",
      "loss at iter 330:0.2649\n",
      "train auc: 0.971257935496\n",
      "test auc: 0.971058300951\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at iter 331:0.2646\n",
      "train auc: 0.971286766755\n",
      "test auc: 0.971081675297\n",
      "loss at iter 332:0.2643\n",
      "train auc: 0.97131556912\n",
      "test auc: 0.971105763987\n",
      "loss at iter 333:0.2641\n",
      "train auc: 0.971344227611\n",
      "test auc: 0.971128692378\n",
      "loss at iter 334:0.2638\n",
      "train auc: 0.971373085856\n",
      "test auc: 0.971153351929\n",
      "loss at iter 335:0.2636\n",
      "train auc: 0.97140197639\n",
      "test auc: 0.971177076814\n",
      "loss at iter 336:0.2633\n",
      "train auc: 0.971431039676\n",
      "test auc: 0.971201037537\n",
      "loss at iter 337:0.2631\n",
      "train auc: 0.971459711891\n",
      "test auc: 0.971223050839\n",
      "loss at iter 338:0.2628\n",
      "train auc: 0.971488368575\n",
      "test auc: 0.971246478128\n",
      "loss at iter 339:0.2625\n",
      "train auc: 0.971516862957\n",
      "test auc: 0.971271211799\n",
      "loss at iter 340:0.2623\n",
      "train auc: 0.971545123574\n",
      "test auc: 0.971295069246\n",
      "loss at iter 341:0.2620\n",
      "train auc: 0.971573592971\n",
      "test auc: 0.97131858144\n",
      "loss at iter 342:0.2618\n",
      "train auc: 0.97160191483\n",
      "test auc: 0.971343108939\n",
      "loss at iter 343:0.2615\n",
      "train auc: 0.971629762094\n",
      "test auc: 0.971367459105\n",
      "loss at iter 344:0.2613\n",
      "train auc: 0.971657755018\n",
      "test auc: 0.971389861972\n",
      "loss at iter 345:0.2610\n",
      "train auc: 0.971685870023\n",
      "test auc: 0.971412566611\n",
      "loss at iter 346:0.2608\n",
      "train auc: 0.971713279462\n",
      "test auc: 0.971435323914\n",
      "loss at iter 347:0.2605\n",
      "train auc: 0.971740680149\n",
      "test auc: 0.971460224599\n",
      "loss at iter 348:0.2603\n",
      "train auc: 0.971768119644\n",
      "test auc: 0.971481156069\n",
      "loss at iter 349:0.2600\n",
      "train auc: 0.971795434015\n",
      "test auc: 0.971505190236\n",
      "loss at iter 350:0.2598\n",
      "train auc: 0.971822492163\n",
      "test auc: 0.971527712096\n",
      "loss at iter 351:0.2595\n",
      "train auc: 0.971849645643\n",
      "test auc: 0.97155048099\n",
      "loss at iter 352:0.2593\n",
      "train auc: 0.971876470155\n",
      "test auc: 0.97157290281\n",
      "loss at iter 353:0.2591\n",
      "train auc: 0.971903340937\n",
      "test auc: 0.971596629813\n",
      "loss at iter 354:0.2588\n",
      "train auc: 0.971930206936\n",
      "test auc: 0.971618497632\n",
      "loss at iter 355:0.2586\n",
      "train auc: 0.971957024937\n",
      "test auc: 0.971639599162\n",
      "loss at iter 356:0.2583\n",
      "train auc: 0.971983879056\n",
      "test auc: 0.971660130988\n",
      "loss at iter 357:0.2581\n",
      "train auc: 0.97201041843\n",
      "test auc: 0.971682813542\n",
      "loss at iter 358:0.2579\n",
      "train auc: 0.972036834461\n",
      "test auc: 0.971705406314\n",
      "loss at iter 359:0.2576\n",
      "train auc: 0.972063027636\n",
      "test auc: 0.971725821968\n",
      "loss at iter 360:0.2574\n",
      "train auc: 0.972088770722\n",
      "test auc: 0.971745780376\n",
      "loss at iter 361:0.2571\n",
      "train auc: 0.972114536686\n",
      "test auc: 0.971768124271\n",
      "loss at iter 362:0.2569\n",
      "train auc: 0.972139915766\n",
      "test auc: 0.971789594141\n",
      "loss at iter 363:0.2567\n",
      "train auc: 0.9721654053\n",
      "test auc: 0.971810399423\n",
      "loss at iter 364:0.2565\n",
      "train auc: 0.97219100542\n",
      "test auc: 0.971832421172\n",
      "loss at iter 365:0.2562\n",
      "train auc: 0.972216342512\n",
      "test auc: 0.971851799298\n",
      "loss at iter 366:0.2560\n",
      "train auc: 0.972241571613\n",
      "test auc: 0.971871695144\n",
      "loss at iter 367:0.2558\n",
      "train auc: 0.972266819695\n",
      "test auc: 0.97189237092\n",
      "loss at iter 368:0.2555\n",
      "train auc: 0.972291985302\n",
      "test auc: 0.971913616293\n",
      "loss at iter 369:0.2553\n",
      "train auc: 0.972317123778\n",
      "test auc: 0.971934213751\n",
      "loss at iter 370:0.2551\n",
      "train auc: 0.972341961568\n",
      "test auc: 0.971954712484\n",
      "loss at iter 371:0.2548\n",
      "train auc: 0.972367068355\n",
      "test auc: 0.971973919044\n",
      "loss at iter 372:0.2546\n",
      "train auc: 0.972391833805\n",
      "test auc: 0.971993553793\n",
      "loss at iter 373:0.2544\n",
      "train auc: 0.972416496113\n",
      "test auc: 0.97201385143\n",
      "loss at iter 374:0.2542\n",
      "train auc: 0.97244107831\n",
      "test auc: 0.972034350513\n",
      "loss at iter 375:0.2539\n",
      "train auc: 0.972465626949\n",
      "test auc: 0.972053297322\n",
      "loss at iter 376:0.2537\n",
      "train auc: 0.972490097328\n",
      "test auc: 0.972074387177\n",
      "loss at iter 377:0.2535\n",
      "train auc: 0.97251460752\n",
      "test auc: 0.972094256294\n",
      "loss at iter 378:0.2533\n",
      "train auc: 0.972538770762\n",
      "test auc: 0.972112384602\n",
      "loss at iter 379:0.2531\n",
      "train auc: 0.972562842332\n",
      "test auc: 0.972133101278\n",
      "loss at iter 380:0.2528\n",
      "train auc: 0.97258702395\n",
      "test auc: 0.972152746414\n",
      "loss at iter 381:0.2526\n",
      "train auc: 0.972610977448\n",
      "test auc: 0.972171991111\n",
      "loss at iter 382:0.2524\n",
      "train auc: 0.972635181876\n",
      "test auc: 0.97219233274\n",
      "loss at iter 383:0.2522\n",
      "train auc: 0.972658924076\n",
      "test auc: 0.972212084391\n",
      "loss at iter 384:0.2520\n",
      "train auc: 0.972683051089\n",
      "test auc: 0.97223202081\n",
      "loss at iter 385:0.2518\n",
      "train auc: 0.97270674724\n",
      "test auc: 0.972251192019\n",
      "loss at iter 386:0.2516\n",
      "train auc: 0.972730799108\n",
      "test auc: 0.972271173877\n",
      "loss at iter 387:0.2513\n",
      "train auc: 0.972754670772\n",
      "test auc: 0.972290862492\n",
      "loss at iter 388:0.2511\n",
      "train auc: 0.972778373399\n",
      "test auc: 0.972311451535\n",
      "loss at iter 389:0.2509\n",
      "train auc: 0.972801961987\n",
      "test auc: 0.972331249717\n",
      "loss at iter 390:0.2507\n",
      "train auc: 0.972825411365\n",
      "test auc: 0.972350428938\n",
      "loss at iter 391:0.2505\n",
      "train auc: 0.972848882677\n",
      "test auc: 0.972370296433\n",
      "loss at iter 392:0.2503\n",
      "train auc: 0.972872169184\n",
      "test auc: 0.972389752423\n",
      "loss at iter 393:0.2501\n",
      "train auc: 0.97289559576\n",
      "test auc: 0.972407824361\n",
      "loss at iter 394:0.2499\n",
      "train auc: 0.972918885481\n",
      "test auc: 0.972427480334\n",
      "loss at iter 395:0.2496\n",
      "train auc: 0.972942157011\n",
      "test auc: 0.972446304091\n",
      "loss at iter 396:0.2494\n",
      "train auc: 0.972965349443\n",
      "test auc: 0.972464966396\n",
      "loss at iter 397:0.2492\n",
      "train auc: 0.972988324597\n",
      "test auc: 0.972483344945\n",
      "loss at iter 398:0.2490\n",
      "train auc: 0.973011518349\n",
      "test auc: 0.972501561932\n",
      "loss at iter 399:0.2488\n",
      "train auc: 0.973034721179\n",
      "test auc: 0.972519773875\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "trainloss = list()\n",
    "testloss  = list()\n",
    "acctrain  = list()\n",
    "acctest   = list()\n",
    "s.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "# run optimizing iterations\n",
    "for i in range(400):\n",
    "    #batchX, batchY = s.run(tf.train.batch([X_train_flat, y_train_oh],100,enqueue_many=True, capacity=1))\n",
    "    s.run(optimizer, {input_X: X_train_flat, input_y: y_train_oh})\n",
    "    #s.run(optimizer, {input_X: batchX, input_y: batchY})\n",
    "    loss_i = s.run(loss, {input_X: X_train_flat, input_y: y_train_oh})\n",
    "    trainloss.append(loss_i)\n",
    "    loss_i = s.run(loss, {input_X: X_test_flat, input_y: y_test_oh})\n",
    "    testloss.append(loss_i)\n",
    "    acctrain.append(s.run(accuracy, feed_dict={input_X:X_train_flat, input_y: y_train_oh}))\n",
    "    acctest.append(s.run(accuracy, feed_dict={input_X:X_test_flat, input_y: y_test_oh}))\n",
    "    print(\"loss at iter %i:%.4f\" % (i, loss_i))\n",
    "    print(\"train auc:\", roc_auc_score(y_train_oh, s.run(predicted_y, {input_X:X_train_flat})))\n",
    "    print(\"test auc:\", roc_auc_score(y_test_oh, s.run(predicted_y, {input_X:X_test_flat})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the plots of loss and accuracy on train and test data for each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEdCAYAAAAPT9w1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc7736890b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEdCAYAAADjFntmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc7549b0d68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(trainloss, label = \"loss train\")\n",
    "plt.plot(testloss, label=\"loss test\")\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('iteration')\n",
    "plt.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc=3,\n",
    "           ncol=2, mode=\"expand\", borderaxespad=0.)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(acctrain, label = \"accuracy train\")\n",
    "plt.plot(acctest, label  = \"accuracy test\")\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('iteration')\n",
    "plt.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc=3,\n",
    "           ncol=2, mode=\"expand\", borderaxespad=0.)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will print the accuracy (proportion of correct predictions) reached after 400 learning steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on train data: 0.92684\n",
      "Accuracy on val data: 0.9254\n",
      "Accuracy on test data: 0.9247\n"
     ]
    }
   ],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(predicted_y,1), tf.argmax(input_y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(\"Accuracy on train data:\", s.run(accuracy, feed_dict={input_X:X_train_flat, input_y: y_train_oh}))\n",
    "print(\"Accuracy on val data:\"  , s.run(accuracy, feed_dict={input_X:X_val_flat, input_y: y_val_oh}))\n",
    "print(\"Accuracy on test data:\" , s.run(accuracy, feed_dict={input_X:X_test_flat, input_y: y_test_oh}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
