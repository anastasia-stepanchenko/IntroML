{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Simplest NN\n",
    "This code builds a 2-layer NN and applies it to recogition of handwritten digits.\n",
    "Tested on Python 3.6.3 (v3.6.3:2c5fed8, Oct  3 2017, 18:11:49) [MSC v.1900 64 bit (AMD64)] on win32,\n",
    "Windows 10 Pro 64 Bit, Intel Core i5-2500 @ 3.30 GHz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we load 'mnist' dataset of handwritten images (https://keras.io/datasets/) with help of an already provided function *load_dataset*. Be sure that the file *preprocessed_mnist.py* is in your working directory.\n",
    "\n",
    "Now we also want to know the shapes of the original train/test-data and visualize some symbols from the train data. We will want to classify each handwritten symbol as a digit: 0,1,...,9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 28, 28) (50000,)\n",
      "(10000, 28, 28) (10000,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x28e377c6eb8>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAADEdJREFUeJzt3W+IXPW9x/HPJ7Z5oA2LMWMarN5N\ni5RK403rsFywFC/FYksxNlBpHpRcKGyRirfSB1dENCCClNt/wqWQXEO20NoWWmseSJsgBW/hKs6q\n1DS5bUX2pmlCstFK7QPxz377YE9kjTtnJjPnzyTf9wuWmTm/M3M+TPLZMzPn7PwcEQKQz5q2AwBo\nB+UHkqL8QFKUH0iK8gNJUX4gKcoPJEX5gaQoP5DU+5rc2IYNG2J6errJTQKpLCws6PTp0x5m3bHK\nb/smSd+XdJGk/46IB8vWn56eVq/XG2eTAEp0u92h1x35Zb/tiyT9l6TPSbpG0g7b14z6eACaNc57\n/hlJL0bESxHxhqSfSNpWTSwAdRun/FdI+vOK28eKZe9ie9Z2z3ZvcXFxjM0BqNI45V/tQ4X3/H1w\nROyOiG5EdDudzhibA1Clccp/TNKVK25/SNLx8eIAaMo45X9G0tW2N9teK+nLkvZXEwtA3UY+1BcR\nb9m+XdKvtXyob29E/L6yZABqNdZx/oh4XNLjFWUB0CBO7wWSovxAUpQfSIryA0lRfiApyg8kRfmB\npCg/kBTlB5Ki/EBSlB9IivIDSVF+ICnKDyRF+YGkKD+QFOUHkqL8QFKUH0iK8gNJUX4gqUan6Ebz\n7r///tLxe++9t3R8ZmamdPzAgQOl41NTU6XjaA97fiApyg8kRfmBpCg/kBTlB5Ki/EBSlB9Iaqzj\n/LYXJL0m6W1Jb0VEt4pQODevvvpq37GHHnqo9L5r1pT//p+fny8dP3r0aOn4li1bSsfRnipO8vnX\niDhdweMAaBAv+4Gkxi1/SDpge972bBWBADRj3Jf910fEcduXSzpo+/8i4smVKxS/FGYl6aqrrhpz\ncwCqMtaePyKOF5enJD0q6T1/BRIRuyOiGxHdTqczzuYAVGjk8tu+xPa6M9clfVbSoaqCAajXOC/7\nN0p61PaZx/lxRPyqklQAajdy+SPiJUn/XGEWjOjiiy/uO3bzzTeX3nffvn0Vp8H5gkN9QFKUH0iK\n8gNJUX4gKcoPJEX5gaT46u4LwNq1a/uObd68ucEkOJ+w5weSovxAUpQfSIryA0lRfiApyg8kRfmB\npDjOfwF4/fXX+44999xzDSbB+YQ9P5AU5QeSovxAUpQfSIryA0lRfiApyg8kxXH+C8Cbb77Zd+zw\n4cO1bvupp54qHS+bom1qaqrqODgH7PmBpCg/kBTlB5Ki/EBSlB9IivIDSVF+IKmBx/lt75X0BUmn\nIuLjxbL1kn4qaVrSgqRbI+Kv9cVEmXXr1vUdu/POO0vve9ttt4217UH3v+yyy/qObd++faxtYzzD\n7Pn3SbrprGV3SXoiIq6W9ERxG8B5ZGD5I+JJSa+ctXibpLni+pykWyrOBaBmo77n3xgRJySpuLy8\nukgAmlD7B362Z233bPcWFxfr3hyAIY1a/pO2N0lScXmq34oRsTsiuhHR7XQ6I24OQNVGLf9+STuL\n6zslPVZNHABNGVh+249I+l9JH7V9zPZXJT0o6Ubbf5J0Y3EbwHlk4HH+iNjRZ+gzFWdBDWZnZ0vH\nxz3Oj/MXZ/gBSVF+ICnKDyRF+YGkKD+QFOUHkuKru5NbWloqHV+zhv3DhYp/WSApyg8kRfmBpCg/\nkBTlB5Ki/EBSlB9IiuP8yQ06jm+7oSRoGnt+ICnKDyRF+YGkKD+QFOUHkqL8QFKUH0iK8gNJUX4g\nKcoPJEX5gaQoP5AU5QeSovxAUpQfSGpg+W3vtX3K9qEVy3bZ/ovt54ufz9cbE0DVhtnz75N00yrL\nvxsRW4ufx6uNBaBuA8sfEU9KeqWBLAAaNM57/ttt/654W3BpZYkANGLU8v9A0kckbZV0QtK3+61o\ne9Z2z3ZvcXFxxM0BqNpI5Y+IkxHxdkQsSdojaaZk3d0R0Y2IbqfTGTUngIqNVH7bm1bc/KKkQ/3W\nBTCZBn51t+1HJN0gaYPtY5Luk3SD7a2SQtKCpK/VmBFADQaWPyJ2rLL44RqyoAVLS0ul44O+13+Q\ngwcP9h3bvn37WI+N8XCGH5AU5QeSovxAUpQfSIryA0lRfiAppuhOru4puvfs2dN3bNeuXaX33bhx\n41jbRjn2/EBSlB9IivIDSVF+ICnKDyRF+YGkKD+QFMf5k7vnnntKxx944IHatl12DoA0OBvGw54f\nSIryA0lRfiApyg8kRfmBpCg/kBTlB5LiOH9y1157bdsR0BL2/EBSlB9IivIDSVF+ICnKDyRF+YGk\nKD+QlCOifAX7Skk/lPRBSUuSdkfE922vl/RTSdOSFiTdGhF/LXusbrcbvV6vgthoypYtW0rHDx8+\nPPJjD5oe/OWXXy4dX79+/cjbvlB1u131er2hJlsYZs//lqRvRsTHJP2LpK/bvkbSXZKeiIirJT1R\n3AZwnhhY/og4ERHPFtdfk3RE0hWStkmaK1abk3RLXSEBVO+c3vPbnpb0CUlPS9oYESek5V8Qki6v\nOhyA+gxdftsfkPRzSd+IiL+dw/1mbfds9xYXF0fJCKAGQ5Xf9vu1XPwfRcQvisUnbW8qxjdJOrXa\nfSNid0R0I6Lb6XSqyAygAgPL7+VpWh+WdCQivrNiaL+kncX1nZIeqz4egLoM8ye910v6iqQXbD9f\nLLtb0oOSfmb7q5KOSvpSPRHRppmZmdLxI0eOjPzYg6YHR70Glj8ifiup33HDz1QbB0BT+NULJEX5\ngaQoP5AU5QeSovxAUpQfSIqv7kapO+64o3R8bm6udByTiz0/kBTlB5Ki/EBSlB9IivIDSVF+ICnK\nDyTFcX6Ump6eLh2/7rrrSsfn5+crTIMqsecHkqL8QFKUH0iK8gNJUX4gKcoPJEX5gaQ4zo9SU1NT\npeNPP/10Q0lQNfb8QFKUH0iK8gNJUX4gKcoPJEX5gaQoP5DUwPLbvtL2b2wfsf172/9eLN9l+y+2\nny9+Pl9/XABVGeYkn7ckfTMinrW9TtK87YPF2Hcj4j/riwegLgPLHxEnJJ0orr9m+4ikK+oOBqBe\n5/Se3/a0pE9IOnNO5+22f2d7r+1L+9xn1nbPdm9xcXGssACqM3T5bX9A0s8lfSMi/ibpB5I+Immr\nll8ZfHu1+0XE7ojoRkS30+lUEBlAFYYqv+33a7n4P4qIX0hSRJyMiLcjYknSHkkz9cUEULVhPu23\npIclHYmI76xYvmnFal+UdKj6eADqMsyn/ddL+oqkF2w/Xyy7W9IO21slhaQFSV+rJSGAWgzzaf9v\nJXmVocerjwOgKZzhByRF+YGkKD+QFOUHkqL8QFKUH0iK8gNJUX4gKcoPJEX5gaQoP5AU5QeSovxA\nUpQfSMoR0dzG7EVJ/79i0QZJpxsLcG4mNduk5pLINqoqs/1TRAz1fXmNlv89G7d7EdFtLUCJSc02\nqbkkso2qrWy87AeSovxAUm2Xf3fL2y8zqdkmNZdEtlG1kq3V9/wA2tP2nh9AS1opv+2bbP/B9ou2\n72ojQz+2F2y/UMw83Gs5y17bp2wfWrFsve2Dtv9UXK46TVpL2SZi5uaSmaVbfe4mbcbrxl/2275I\n0h8l3SjpmKRnJO2IiMONBunD9oKkbkS0fkzY9qcl/V3SDyPi48Wyb0l6JSIeLH5xXhoR/zEh2XZJ\n+nvbMzcXE8psWjmztKRbJP2bWnzuSnLdqhaetzb2/DOSXoyIlyLiDUk/kbSthRwTLyKelPTKWYu3\nSZorrs9p+T9P4/pkmwgRcSIini2uvybpzMzSrT53Jbla0Ub5r5D05xW3j2mypvwOSQdsz9uebTvM\nKjYW06afmT798pbznG3gzM1NOmtm6Yl57kaZ8bpqbZR/tdl/JumQw/UR8UlJn5P09eLlLYYz1MzN\nTVllZumJMOqM11Vro/zHJF254vaHJB1vIceqIuJ4cXlK0qOavNmHT56ZJLW4PNVynndM0szNq80s\nrQl47iZpxus2yv+MpKttb7a9VtKXJe1vIcd72L6k+CBGti+R9FlN3uzD+yXtLK7vlPRYi1neZVJm\nbu43s7Rafu4mbcbrVk7yKQ5lfE/SRZL2RsQDjYdYhe0Pa3lvLy1PYvrjNrPZfkTSDVr+q6+Tku6T\n9EtJP5N0laSjkr4UEY1/8NYn2w1afun6zszNZ95jN5ztU5L+R9ILkpaKxXdr+f11a89dSa4dauF5\n4ww/ICnO8AOSovxAUpQfSIryA0lRfiApyg8kRfmBpCg/kNQ/AA/yagYXCqVvAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x28e37fb3da0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# os.chdir(\"D:\\\\Downloads\\\\\")\n",
    "from preprocessed_mnist import load_dataset\n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_dataset()\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.imshow(X_train[5], cmap=\"Greys\")\n",
    "plt.imshow(X_train[6], cmap=\"Greys\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want the image data to be in a flat format, so let's reload the reshaped version of the dataset and print new shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X and y train shape: (50000, 784) (50000,)\n",
      "X and y test shape: (10000, 784) (10000,)\n"
     ]
    }
   ],
   "source": [
    "X_train_flat, y_train, X_val_flat, y_val, X_test_flat, y_test = load_dataset(flatten=True)\n",
    "print(\"X and y train shape:\", X_train_flat.shape, y_train.shape)\n",
    "print(\"X and y test shape:\", X_test_flat.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also want to convert categorical labels of 10 classes to binary format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = tf.Session()\n",
    "\n",
    "# Categorical labels to binaries\n",
    "y_train_oh = s.run(tf.one_hot(y_train, 10))\n",
    "y_test_oh  = s.run(tf.one_hot(y_test, 10))\n",
    "y_val_oh   = s.run(tf.one_hot(y_val, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression\n",
    "Here we will implement our simple NN with 1 hidden layer having 50 neurons.\n",
    "\n",
    "First let's create variables for weights and biases and placeholders for input data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'Placeholder:0' shape=(?, 784) dtype=float32>,\n",
       " <tf.Tensor 'Placeholder_1:0' shape=(?, 10) dtype=float32>,\n",
       " <tf.Variable 'weights_h:0' shape=(784, 50) dtype=float32_ref>,\n",
       " <tf.Variable 'weights:0' shape=(50, 10) dtype=float32_ref>,\n",
       " <tf.Variable 'biases_h:0' shape=(50,) dtype=float32_ref>,\n",
       " <tf.Variable 'biases:0' shape=(10,) dtype=float32_ref>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model parameters - weights and bias\n",
    "nhid   = 50\n",
    "nclass = len(np.unique(y_train))\n",
    "weights_hid = tf.Variable(tf.random_normal([X_train_flat.shape[1], nhid], stddev=0.35),\n",
    "                      name=\"weights_h\") \n",
    "\n",
    "b_hid = tf.Variable(tf.zeros([nhid]), dtype='float32', name=\"biases_h\")\n",
    "\n",
    "weights_out = tf.Variable(tf.random_normal([nhid, nclass], stddev=0.35),\n",
    "                      name=\"weights\") \n",
    "\n",
    "b_out = tf.Variable(tf.zeros([nclass]), dtype='float32', name=\"biases\")\n",
    "\n",
    "# Placeholders for the input data\n",
    "input_X = tf.placeholder('float32', shape=(None,X_train_flat.shape[1]))\n",
    "input_y = tf.placeholder('float32', shape=(None, nclass))\n",
    "input_X, input_y, weights_hid, weights_out, b_hid, b_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now implement the model itself. The outpit layer uses softmax non-linearity, hidden layer - *relu* activation function.\n",
    "\n",
    "**Loss** will be a softmax cross entropy between log probabilities and labels.  \n",
    "**Optimization method** - gradient decent.  \n",
    "**Accuracy** is just a proportion of correct predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "#predicted_y =  tf.nn.softmax(tf.matmul(input_X, weights)+b)\n",
    "predicted_y_hid =  tf.nn.relu(tf.matmul(input_X, weights_hid)+b_hid)\n",
    "predicted_y     =  tf.matmul(predicted_y_hid, weights_out)+b_out\n",
    "\n",
    "# Loss. Should be a scalar number - average loss over all the objects\n",
    "#loss = tf.reduce_mean(-tf.reduce_sum(tf.log(predicted_y+1e-07)*input_y, reduction_indices=[1]))\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=input_y, logits=predicted_y))\n",
    "\n",
    "# See above for an example. tf.train.*Optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.5).minimize(\n",
    "    loss, var_list=(weights_hid, b_hid, weights_out, b_out))\n",
    "\n",
    "# compute accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(predicted_y,1), tf.argmax(input_y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will make 400 learning iterations and also show loss, auc on train and test data for each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at iter 0:6.8765\n",
      "train auc: 0.687510221914\n",
      "test auc: 0.687148972178\n",
      "loss at iter 1:4.1253\n",
      "train auc: 0.737293261747\n",
      "test auc: 0.74093541607\n",
      "loss at iter 2:2.3339\n",
      "train auc: 0.780183826451\n",
      "test auc: 0.78419227763\n",
      "loss at iter 3:1.9347\n",
      "train auc: 0.794150118034\n",
      "test auc: 0.797733909147\n",
      "loss at iter 4:1.7506\n",
      "train auc: 0.808858981724\n",
      "test auc: 0.812266050027\n",
      "loss at iter 5:1.6170\n",
      "train auc: 0.82165356534\n",
      "test auc: 0.8248844439\n",
      "loss at iter 6:1.5041\n",
      "train auc: 0.833185658418\n",
      "test auc: 0.836169214545\n",
      "loss at iter 7:1.4038\n",
      "train auc: 0.843643586786\n",
      "test auc: 0.846442356625\n",
      "loss at iter 8:1.3141\n",
      "train auc: 0.853183937459\n",
      "test auc: 0.855877009768\n",
      "loss at iter 9:1.2351\n",
      "train auc: 0.861926979035\n",
      "test auc: 0.864690397658\n",
      "loss at iter 10:1.1669\n",
      "train auc: 0.8698987913\n",
      "test auc: 0.872727050124\n",
      "loss at iter 11:1.1077\n",
      "train auc: 0.877120249253\n",
      "test auc: 0.879969099024\n",
      "loss at iter 12:1.0555\n",
      "train auc: 0.883507153124\n",
      "test auc: 0.886405002165\n",
      "loss at iter 13:1.0092\n",
      "train auc: 0.889069499892\n",
      "test auc: 0.89203147187\n",
      "loss at iter 14:0.9680\n",
      "train auc: 0.893949972889\n",
      "test auc: 0.896940000606\n",
      "loss at iter 15:0.9312\n",
      "train auc: 0.898223621215\n",
      "test auc: 0.901246968926\n",
      "loss at iter 16:0.8982\n",
      "train auc: 0.902007419909\n",
      "test auc: 0.905062542055\n",
      "loss at iter 17:0.8685\n",
      "train auc: 0.905389547808\n",
      "test auc: 0.908451597844\n",
      "loss at iter 18:0.8419\n",
      "train auc: 0.90840129426\n",
      "test auc: 0.911461352272\n",
      "loss at iter 19:0.8178\n",
      "train auc: 0.91110057259\n",
      "test auc: 0.914156262065\n",
      "loss at iter 20:0.7961\n",
      "train auc: 0.913528594005\n",
      "test auc: 0.916579522576\n",
      "loss at iter 21:0.7761\n",
      "train auc: 0.915719389484\n",
      "test auc: 0.918775900695\n",
      "loss at iter 22:0.7578\n",
      "train auc: 0.917705092525\n",
      "test auc: 0.92077437574\n",
      "loss at iter 23:0.7410\n",
      "train auc: 0.91951515418\n",
      "test auc: 0.922589069999\n",
      "loss at iter 24:0.7255\n",
      "train auc: 0.921184463969\n",
      "test auc: 0.924247708785\n",
      "loss at iter 25:0.7112\n",
      "train auc: 0.922713483833\n",
      "test auc: 0.92576875362\n",
      "loss at iter 26:0.6980\n",
      "train auc: 0.924118861664\n",
      "test auc: 0.927165882993\n",
      "loss at iter 27:0.6856\n",
      "train auc: 0.925427176217\n",
      "test auc: 0.928468551065\n",
      "loss at iter 28:0.6741\n",
      "train auc: 0.926635694903\n",
      "test auc: 0.9296697283\n",
      "loss at iter 29:0.6633\n",
      "train auc: 0.927763226406\n",
      "test auc: 0.930795339185\n",
      "loss at iter 30:0.6531\n",
      "train auc: 0.928810420962\n",
      "test auc: 0.931842175261\n",
      "loss at iter 31:0.6436\n",
      "train auc: 0.929792680075\n",
      "test auc: 0.932809733424\n",
      "loss at iter 32:0.6346\n",
      "train auc: 0.930711053024\n",
      "test auc: 0.933715941349\n",
      "loss at iter 33:0.6260\n",
      "train auc: 0.931573332273\n",
      "test auc: 0.934572790608\n",
      "loss at iter 34:0.6179\n",
      "train auc: 0.932386596351\n",
      "test auc: 0.935379836693\n",
      "loss at iter 35:0.6102\n",
      "train auc: 0.933155282661\n",
      "test auc: 0.936137926245\n",
      "loss at iter 36:0.6029\n",
      "train auc: 0.933882434578\n",
      "test auc: 0.936851113905\n",
      "loss at iter 37:0.5959\n",
      "train auc: 0.934569461486\n",
      "test auc: 0.937520032986\n",
      "loss at iter 38:0.5893\n",
      "train auc: 0.935223024506\n",
      "test auc: 0.938151924223\n",
      "loss at iter 39:0.5830\n",
      "train auc: 0.935842448782\n",
      "test auc: 0.938755599668\n",
      "loss at iter 40:0.5770\n",
      "train auc: 0.936434312298\n",
      "test auc: 0.939330105059\n",
      "loss at iter 41:0.5712\n",
      "train auc: 0.936997660222\n",
      "test auc: 0.939873175472\n",
      "loss at iter 42:0.5657\n",
      "train auc: 0.937534920823\n",
      "test auc: 0.940391600082\n",
      "loss at iter 43:0.5603\n",
      "train auc: 0.938048012671\n",
      "test auc: 0.940890775923\n",
      "loss at iter 44:0.5552\n",
      "train auc: 0.938541247396\n",
      "test auc: 0.941363928509\n",
      "loss at iter 45:0.5502\n",
      "train auc: 0.93901221402\n",
      "test auc: 0.941818441836\n",
      "loss at iter 46:0.5454\n",
      "train auc: 0.939464256657\n",
      "test auc: 0.942254424055\n",
      "loss at iter 47:0.5408\n",
      "train auc: 0.939897104791\n",
      "test auc: 0.942676514963\n",
      "loss at iter 48:0.5364\n",
      "train auc: 0.940314928505\n",
      "test auc: 0.943072758826\n",
      "loss at iter 49:0.5321\n",
      "train auc: 0.940716288703\n",
      "test auc: 0.943454709103\n",
      "loss at iter 50:0.5280\n",
      "train auc: 0.94110420893\n",
      "test auc: 0.943826178006\n",
      "loss at iter 51:0.5240\n",
      "train auc: 0.941478123419\n",
      "test auc: 0.944184488123\n",
      "loss at iter 52:0.5201\n",
      "train auc: 0.941840165912\n",
      "test auc: 0.944532749422\n",
      "loss at iter 53:0.5164\n",
      "train auc: 0.942190532202\n",
      "test auc: 0.944863951656\n",
      "loss at iter 54:0.5128\n",
      "train auc: 0.942528245138\n",
      "test auc: 0.945185193358\n",
      "loss at iter 55:0.5093\n",
      "train auc: 0.942855334219\n",
      "test auc: 0.945497260882\n",
      "loss at iter 56:0.5058\n",
      "train auc: 0.943173259768\n",
      "test auc: 0.945796170993\n",
      "loss at iter 57:0.5025\n",
      "train auc: 0.943481253898\n",
      "test auc: 0.946084560981\n",
      "loss at iter 58:0.4993\n",
      "train auc: 0.943780206294\n",
      "test auc: 0.946370014124\n",
      "loss at iter 59:0.4962\n",
      "train auc: 0.944070991007\n",
      "test auc: 0.946646814165\n",
      "loss at iter 60:0.4931\n",
      "train auc: 0.944353125408\n",
      "test auc: 0.946912826443\n",
      "loss at iter 61:0.4902\n",
      "train auc: 0.94462709269\n",
      "test auc: 0.947170594498\n",
      "loss at iter 62:0.4873\n",
      "train auc: 0.944895899686\n",
      "test auc: 0.947421014293\n",
      "loss at iter 63:0.4845\n",
      "train auc: 0.945157994627\n",
      "test auc: 0.947666623163\n",
      "loss at iter 64:0.4817\n",
      "train auc: 0.945412984672\n",
      "test auc: 0.947907396248\n",
      "loss at iter 65:0.4791\n",
      "train auc: 0.945662887066\n",
      "test auc: 0.948141201994\n",
      "loss at iter 66:0.4764\n",
      "train auc: 0.945908032837\n",
      "test auc: 0.948371194358\n",
      "loss at iter 67:0.4739\n",
      "train auc: 0.946148411685\n",
      "test auc: 0.948600677765\n",
      "loss at iter 68:0.4714\n",
      "train auc: 0.946383117914\n",
      "test auc: 0.94881961645\n",
      "loss at iter 69:0.4690\n",
      "train auc: 0.946612060832\n",
      "test auc: 0.949032791818\n",
      "loss at iter 70:0.4666\n",
      "train auc: 0.946838159342\n",
      "test auc: 0.949248626408\n",
      "loss at iter 71:0.4642\n",
      "train auc: 0.94705789301\n",
      "test auc: 0.949457835276\n",
      "loss at iter 72:0.4619\n",
      "train auc: 0.947272436336\n",
      "test auc: 0.9496592818\n",
      "loss at iter 73:0.4596\n",
      "train auc: 0.947484028815\n",
      "test auc: 0.949861890793\n",
      "loss at iter 74:0.4574\n",
      "train auc: 0.947691447776\n",
      "test auc: 0.95005732634\n",
      "loss at iter 75:0.4553\n",
      "train auc: 0.947893679128\n",
      "test auc: 0.950247133819\n",
      "loss at iter 76:0.4532\n",
      "train auc: 0.948093351354\n",
      "test auc: 0.95043414102\n",
      "loss at iter 77:0.4511\n",
      "train auc: 0.948290016124\n",
      "test auc: 0.950617098952\n",
      "loss at iter 78:0.4491\n",
      "train auc: 0.948481882291\n",
      "test auc: 0.950797312671\n",
      "loss at iter 79:0.4471\n",
      "train auc: 0.948671617444\n",
      "test auc: 0.950973996855\n",
      "loss at iter 80:0.4451\n",
      "train auc: 0.948857787315\n",
      "test auc: 0.951142775744\n",
      "loss at iter 81:0.4432\n",
      "train auc: 0.949040413804\n",
      "test auc: 0.951309417107\n",
      "loss at iter 82:0.4413\n",
      "train auc: 0.949218377537\n",
      "test auc: 0.951473843046\n",
      "loss at iter 83:0.4395\n",
      "train auc: 0.94939513203\n",
      "test auc: 0.951636915252\n",
      "loss at iter 84:0.4376\n",
      "train auc: 0.949568275621\n",
      "test auc: 0.951797245331\n",
      "loss at iter 85:0.4359\n",
      "train auc: 0.949738016951\n",
      "test auc: 0.951951752064\n",
      "loss at iter 86:0.4341\n",
      "train auc: 0.949905591487\n",
      "test auc: 0.952109844649\n",
      "loss at iter 87:0.4324\n",
      "train auc: 0.950068739828\n",
      "test auc: 0.952261775604\n",
      "loss at iter 88:0.4307\n",
      "train auc: 0.950230760478\n",
      "test auc: 0.952411125608\n",
      "loss at iter 89:0.4290\n",
      "train auc: 0.950387250408\n",
      "test auc: 0.95255691025\n",
      "loss at iter 90:0.4274\n",
      "train auc: 0.950542996283\n",
      "test auc: 0.95270513251\n",
      "loss at iter 91:0.4258\n",
      "train auc: 0.950696520786\n",
      "test auc: 0.952847886523\n",
      "loss at iter 92:0.4242\n",
      "train auc: 0.95084827185\n",
      "test auc: 0.95299111488\n",
      "loss at iter 93:0.4227\n",
      "train auc: 0.950997202883\n",
      "test auc: 0.953129693965\n",
      "loss at iter 94:0.4211\n",
      "train auc: 0.951144501043\n",
      "test auc: 0.953267570172\n",
      "loss at iter 95:0.4196\n",
      "train auc: 0.951289241175\n",
      "test auc: 0.953400584042\n",
      "loss at iter 96:0.4181\n",
      "train auc: 0.951432377673\n",
      "test auc: 0.953533746142\n",
      "loss at iter 97:0.4167\n",
      "train auc: 0.951572173595\n",
      "test auc: 0.953663854263\n",
      "loss at iter 98:0.4153\n",
      "train auc: 0.951710730332\n",
      "test auc: 0.953792371053\n",
      "loss at iter 99:0.4139\n",
      "train auc: 0.951846876794\n",
      "test auc: 0.953916544218\n",
      "loss at iter 100:0.4124\n",
      "train auc: 0.951983496365\n",
      "test auc: 0.954042942795\n",
      "loss at iter 101:0.4111\n",
      "train auc: 0.952115292015\n",
      "test auc: 0.954163743823\n",
      "loss at iter 102:0.4097\n",
      "train auc: 0.952248189051\n",
      "test auc: 0.954290047373\n",
      "loss at iter 103:0.4084\n",
      "train auc: 0.952374990219\n",
      "test auc: 0.954404619144\n",
      "loss at iter 104:0.4071\n",
      "train auc: 0.95250423172\n",
      "test auc: 0.954523874521\n",
      "loss at iter 105:0.4058\n",
      "train auc: 0.952626219696\n",
      "test auc: 0.954632460557\n",
      "loss at iter 106:0.4045\n",
      "train auc: 0.952753612777\n",
      "test auc: 0.954752702527\n",
      "loss at iter 107:0.4033\n",
      "train auc: 0.952872136001\n",
      "test auc: 0.954859762292\n",
      "loss at iter 108:0.4020\n",
      "train auc: 0.95299783449\n",
      "test auc: 0.954980429997\n",
      "loss at iter 109:0.4008\n",
      "train auc: 0.953112252075\n",
      "test auc: 0.955083358248\n",
      "loss at iter 110:0.3995\n",
      "train auc: 0.953236408299\n",
      "test auc: 0.955202040926\n",
      "loss at iter 111:0.3984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train auc: 0.953346915579\n",
      "test auc: 0.955300570577\n",
      "loss at iter 112:0.3971\n",
      "train auc: 0.953469286446\n",
      "test auc: 0.955417351302\n",
      "loss at iter 113:0.3961\n",
      "train auc: 0.953573979252\n",
      "test auc: 0.955507890614\n",
      "loss at iter 114:0.3948\n",
      "train auc: 0.953695746138\n",
      "test auc: 0.955622041934\n",
      "loss at iter 115:0.3939\n",
      "train auc: 0.953793632991\n",
      "test auc: 0.955706903418\n",
      "loss at iter 116:0.3925\n",
      "train auc: 0.95391722033\n",
      "test auc: 0.955824068852\n",
      "loss at iter 117:0.3917\n",
      "train auc: 0.954008576576\n",
      "test auc: 0.955900736111\n",
      "loss at iter 118:0.3904\n",
      "train auc: 0.954134720101\n",
      "test auc: 0.956024237156\n",
      "loss at iter 119:0.3897\n",
      "train auc: 0.954217549217\n",
      "test auc: 0.956088912053\n",
      "loss at iter 120:0.3883\n",
      "train auc: 0.954350538409\n",
      "test auc: 0.956220895013\n",
      "loss at iter 121:0.3878\n",
      "train auc: 0.954422310014\n",
      "test auc: 0.956271435642\n",
      "loss at iter 122:0.3863\n",
      "train auc: 0.954563826919\n",
      "test auc: 0.956417313922\n",
      "loss at iter 123:0.3862\n",
      "train auc: 0.954621764544\n",
      "test auc: 0.956451672387\n",
      "loss at iter 124:0.3846\n",
      "train auc: 0.954777521191\n",
      "test auc: 0.956616463693\n",
      "loss at iter 125:0.3849\n",
      "train auc: 0.9548144247\n",
      "test auc: 0.956624295511\n",
      "loss at iter 126:0.3832\n",
      "train auc: 0.954988205394\n",
      "test auc: 0.956818205122\n",
      "loss at iter 127:0.3844\n",
      "train auc: 0.954998668855\n",
      "test auc: 0.956784215439\n",
      "loss at iter 128:0.3827\n",
      "train auc: 0.955200247533\n",
      "test auc: 0.957022049014\n",
      "loss at iter 129:0.3852\n",
      "train auc: 0.955172010032\n",
      "test auc: 0.956935614208\n",
      "loss at iter 130:0.3838\n",
      "train auc: 0.955415963079\n",
      "test auc: 0.95722864777\n",
      "loss at iter 131:0.3885\n",
      "train auc: 0.955333651633\n",
      "test auc: 0.957068488673\n",
      "loss at iter 132:0.3877\n",
      "train auc: 0.955634415139\n",
      "test auc: 0.957446249305\n",
      "loss at iter 133:0.3966\n",
      "train auc: 0.955475069959\n",
      "test auc: 0.957181261511\n",
      "loss at iter 134:0.3968\n",
      "train auc: 0.95585966607\n",
      "test auc: 0.957671585061\n",
      "loss at iter 135:0.4121\n",
      "train auc: 0.955591445366\n",
      "test auc: 0.957268954523\n",
      "loss at iter 136:0.4118\n",
      "train auc: 0.956083855489\n",
      "test auc: 0.957904436576\n",
      "loss at iter 137:0.4340\n",
      "train auc: 0.955689705925\n",
      "test auc: 0.957350331793\n",
      "loss at iter 138:0.4259\n",
      "train auc: 0.956302003642\n",
      "test auc: 0.958121868099\n",
      "loss at iter 139:0.4451\n",
      "train auc: 0.955848734851\n",
      "test auc: 0.957506183836\n",
      "loss at iter 140:0.4207\n",
      "train auc: 0.956513574694\n",
      "test auc: 0.958316903252\n",
      "loss at iter 141:0.4240\n",
      "train auc: 0.956172808027\n",
      "test auc: 0.957844386642\n",
      "loss at iter 142:0.3969\n",
      "train auc: 0.956725611075\n",
      "test auc: 0.958493987863\n",
      "loss at iter 143:0.3922\n",
      "train auc: 0.956558561935\n",
      "test auc: 0.9582479214\n",
      "loss at iter 144:0.3773\n",
      "train auc: 0.956921796938\n",
      "test auc: 0.958653377482\n",
      "loss at iter 145:0.3744\n",
      "train auc: 0.956865232331\n",
      "test auc: 0.958553600567\n",
      "loss at iter 146:0.3683\n",
      "train auc: 0.957100703384\n",
      "test auc: 0.958809360516\n",
      "loss at iter 147:0.3671\n",
      "train auc: 0.957103554343\n",
      "test auc: 0.958783671499\n",
      "loss at iter 148:0.3643\n",
      "train auc: 0.957272999137\n",
      "test auc: 0.958957094847\n",
      "loss at iter 149:0.3636\n",
      "train auc: 0.957311216282\n",
      "test auc: 0.958975110492\n",
      "loss at iter 150:0.3619\n",
      "train auc: 0.957446248108\n",
      "test auc: 0.959111445624\n",
      "loss at iter 151:0.3612\n",
      "train auc: 0.957502988541\n",
      "test auc: 0.959154946131\n",
      "loss at iter 152:0.3600\n",
      "train auc: 0.95761728097\n",
      "test auc: 0.959266584858\n",
      "loss at iter 153:0.3593\n",
      "train auc: 0.957684572737\n",
      "test auc: 0.959320499448\n",
      "loss at iter 154:0.3583\n",
      "train auc: 0.957786647291\n",
      "test auc: 0.959419139957\n",
      "loss at iter 155:0.3576\n",
      "train auc: 0.957857652093\n",
      "test auc: 0.959478789317\n",
      "loss at iter 156:0.3567\n",
      "train auc: 0.95795212011\n",
      "test auc: 0.959566763376\n",
      "loss at iter 157:0.3561\n",
      "train auc: 0.958026016841\n",
      "test auc: 0.959630422427\n",
      "loss at iter 158:0.3552\n",
      "train auc: 0.958115119859\n",
      "test auc: 0.959716477289\n",
      "loss at iter 159:0.3545\n",
      "train auc: 0.958189452513\n",
      "test auc: 0.959783492263\n",
      "loss at iter 160:0.3537\n",
      "train auc: 0.958273639489\n",
      "test auc: 0.95986089475\n",
      "loss at iter 161:0.3530\n",
      "train auc: 0.958348187667\n",
      "test auc: 0.95992880443\n",
      "loss at iter 162:0.3523\n",
      "train auc: 0.958429558594\n",
      "test auc: 0.960004148256\n",
      "loss at iter 163:0.3516\n",
      "train auc: 0.958502707803\n",
      "test auc: 0.96006859103\n",
      "loss at iter 164:0.3508\n",
      "train auc: 0.958581267703\n",
      "test auc: 0.960141901211\n",
      "loss at iter 165:0.3502\n",
      "train auc: 0.958653768978\n",
      "test auc: 0.960204640946\n",
      "loss at iter 166:0.3494\n",
      "train auc: 0.958729548898\n",
      "test auc: 0.960277873211\n",
      "loss at iter 167:0.3488\n",
      "train auc: 0.958801332425\n",
      "test auc: 0.96034159862\n",
      "loss at iter 168:0.3481\n",
      "train auc: 0.958875668217\n",
      "test auc: 0.960410566256\n",
      "loss at iter 169:0.3474\n",
      "train auc: 0.958946852343\n",
      "test auc: 0.960473390737\n",
      "loss at iter 170:0.3467\n",
      "train auc: 0.959019134245\n",
      "test auc: 0.960539524788\n",
      "loss at iter 171:0.3461\n",
      "train auc: 0.959088718069\n",
      "test auc: 0.960601374213\n",
      "loss at iter 172:0.3454\n",
      "train auc: 0.959160335924\n",
      "test auc: 0.960666435682\n",
      "loss at iter 173:0.3448\n",
      "train auc: 0.959229437221\n",
      "test auc: 0.960727611206\n",
      "loss at iter 174:0.3441\n",
      "train auc: 0.959299981475\n",
      "test auc: 0.960791612345\n",
      "loss at iter 175:0.3435\n",
      "train auc: 0.959368281029\n",
      "test auc: 0.960853733694\n",
      "loss at iter 176:0.3428\n",
      "train auc: 0.959437580189\n",
      "test auc: 0.960918153127\n",
      "loss at iter 177:0.3422\n",
      "train auc: 0.959504788045\n",
      "test auc: 0.960978182348\n",
      "loss at iter 178:0.3416\n",
      "train auc: 0.959572558651\n",
      "test auc: 0.961042010419\n",
      "loss at iter 179:0.3409\n",
      "train auc: 0.959639693351\n",
      "test auc: 0.96110181639\n",
      "loss at iter 180:0.3403\n",
      "train auc: 0.959707488552\n",
      "test auc: 0.961166485251\n",
      "loss at iter 181:0.3397\n",
      "train auc: 0.959773912318\n",
      "test auc: 0.961225551025\n",
      "loss at iter 182:0.3391\n",
      "train auc: 0.9598407401\n",
      "test auc: 0.961287388345\n",
      "loss at iter 183:0.3385\n",
      "train auc: 0.959905703203\n",
      "test auc: 0.961345242304\n",
      "loss at iter 184:0.3379\n",
      "train auc: 0.959971103043\n",
      "test auc: 0.961407767254\n",
      "loss at iter 185:0.3373\n",
      "train auc: 0.960035800068\n",
      "test auc: 0.961466908314\n",
      "loss at iter 186:0.3367\n",
      "train auc: 0.960100123276\n",
      "test auc: 0.961525477909\n",
      "loss at iter 187:0.3361\n",
      "train auc: 0.960162603429\n",
      "test auc: 0.961584558183\n",
      "loss at iter 188:0.3355\n",
      "train auc: 0.960226193248\n",
      "test auc: 0.961645253163\n",
      "loss at iter 189:0.3350\n",
      "train auc: 0.960288679881\n",
      "test auc: 0.961700858127\n",
      "loss at iter 190:0.3344\n",
      "train auc: 0.960351062607\n",
      "test auc: 0.96175608535\n",
      "loss at iter 191:0.3338\n",
      "train auc: 0.96041273435\n",
      "test auc: 0.961812077131\n",
      "loss at iter 192:0.3333\n",
      "train auc: 0.960474449797\n",
      "test auc: 0.961869319785\n",
      "loss at iter 193:0.3327\n",
      "train auc: 0.960535462861\n",
      "test auc: 0.961921972691\n",
      "loss at iter 194:0.3321\n",
      "train auc: 0.960596355829\n",
      "test auc: 0.961978928164\n",
      "loss at iter 195:0.3316\n",
      "train auc: 0.960655883859\n",
      "test auc: 0.962034106777\n",
      "loss at iter 196:0.3310\n",
      "train auc: 0.96071644663\n",
      "test auc: 0.96208866558\n",
      "loss at iter 197:0.3305\n",
      "train auc: 0.960776075656\n",
      "test auc: 0.962140550453\n",
      "loss at iter 198:0.3300\n",
      "train auc: 0.96083591276\n",
      "test auc: 0.962195290041\n",
      "loss at iter 199:0.3294\n",
      "train auc: 0.960894979758\n",
      "test auc: 0.962249081594\n",
      "loss at iter 200:0.3289\n",
      "train auc: 0.960953725016\n",
      "test auc: 0.962305220926\n",
      "loss at iter 201:0.3284\n",
      "train auc: 0.961012388756\n",
      "test auc: 0.962359632497\n",
      "loss at iter 202:0.3278\n",
      "train auc: 0.961070760494\n",
      "test auc: 0.962413470352\n",
      "loss at iter 203:0.3273\n",
      "train auc: 0.96112820981\n",
      "test auc: 0.962465361011\n",
      "loss at iter 204:0.3268\n",
      "train auc: 0.961185682793\n",
      "test auc: 0.962518705221\n",
      "loss at iter 205:0.3263\n",
      "train auc: 0.961242670409\n",
      "test auc: 0.962571332075\n",
      "loss at iter 206:0.3258\n",
      "train auc: 0.961299492715\n",
      "test auc: 0.962623395238\n",
      "loss at iter 207:0.3253\n",
      "train auc: 0.961356737614\n",
      "test auc: 0.962674759272\n",
      "loss at iter 208:0.3248\n",
      "train auc: 0.961413427349\n",
      "test auc: 0.962725788121\n",
      "loss at iter 209:0.3243\n",
      "train auc: 0.961469680458\n",
      "test auc: 0.962779095822\n",
      "loss at iter 210:0.3237\n",
      "train auc: 0.961526269111\n",
      "test auc: 0.962831060487\n",
      "loss at iter 211:0.3233\n",
      "train auc: 0.961582037365\n",
      "test auc: 0.962881973515\n",
      "loss at iter 212:0.3228\n",
      "train auc: 0.961637632544\n",
      "test auc: 0.962932238201\n",
      "loss at iter 213:0.3223\n",
      "train auc: 0.961692625227\n",
      "test auc: 0.962981857587\n",
      "loss at iter 214:0.3218\n",
      "train auc: 0.961747875791\n",
      "test auc: 0.963032683015\n",
      "loss at iter 215:0.3213\n",
      "train auc: 0.961802263492\n",
      "test auc: 0.963080892345\n",
      "loss at iter 216:0.3208\n",
      "train auc: 0.961856810132\n",
      "test auc: 0.963131365754\n",
      "loss at iter 217:0.3203\n",
      "train auc: 0.961910652586\n",
      "test auc: 0.963180773159\n",
      "loss at iter 218:0.3199\n",
      "train auc: 0.961964101813\n",
      "test auc: 0.963232676936\n",
      "loss at iter 219:0.3194\n",
      "train auc: 0.962016998523\n",
      "test auc: 0.963282630862\n",
      "loss at iter 220:0.3189\n",
      "train auc: 0.962070890702\n",
      "test auc: 0.963330669076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at iter 221:0.3184\n",
      "train auc: 0.962123448711\n",
      "test auc: 0.963377267132\n",
      "loss at iter 222:0.3180\n",
      "train auc: 0.962176201286\n",
      "test auc: 0.963426060582\n",
      "loss at iter 223:0.3175\n",
      "train auc: 0.962228923297\n",
      "test auc: 0.963473629634\n",
      "loss at iter 224:0.3171\n",
      "train auc: 0.962281164571\n",
      "test auc: 0.963520024982\n",
      "loss at iter 225:0.3166\n",
      "train auc: 0.962332955571\n",
      "test auc: 0.963567777389\n",
      "loss at iter 226:0.3161\n",
      "train auc: 0.962385329307\n",
      "test auc: 0.963614723203\n",
      "loss at iter 227:0.3157\n",
      "train auc: 0.962437595352\n",
      "test auc: 0.963661598405\n",
      "loss at iter 228:0.3152\n",
      "train auc: 0.96248980897\n",
      "test auc: 0.963709524044\n",
      "loss at iter 229:0.3148\n",
      "train auc: 0.962541489068\n",
      "test auc: 0.963756495004\n",
      "loss at iter 230:0.3143\n",
      "train auc: 0.962593115096\n",
      "test auc: 0.96380498635\n",
      "loss at iter 231:0.3139\n",
      "train auc: 0.962644577688\n",
      "test auc: 0.963852541108\n",
      "loss at iter 232:0.3134\n",
      "train auc: 0.962696080006\n",
      "test auc: 0.963898905308\n",
      "loss at iter 233:0.3130\n",
      "train auc: 0.962747130446\n",
      "test auc: 0.963944999497\n",
      "loss at iter 234:0.3125\n",
      "train auc: 0.962797367258\n",
      "test auc: 0.963991443461\n",
      "loss at iter 235:0.3121\n",
      "train auc: 0.96284812025\n",
      "test auc: 0.964037759399\n",
      "loss at iter 236:0.3117\n",
      "train auc: 0.962898305621\n",
      "test auc: 0.964085002897\n",
      "loss at iter 237:0.3112\n",
      "train auc: 0.962948468893\n",
      "test auc: 0.964130952059\n",
      "loss at iter 238:0.3108\n",
      "train auc: 0.962998380346\n",
      "test auc: 0.964173880404\n",
      "loss at iter 239:0.3104\n",
      "train auc: 0.963047535528\n",
      "test auc: 0.964218317202\n",
      "loss at iter 240:0.3100\n",
      "train auc: 0.963096223497\n",
      "test auc: 0.964263336134\n",
      "loss at iter 241:0.3095\n",
      "train auc: 0.963145179462\n",
      "test auc: 0.964308010066\n",
      "loss at iter 242:0.3091\n",
      "train auc: 0.963193669812\n",
      "test auc: 0.964353826702\n",
      "loss at iter 243:0.3087\n",
      "train auc: 0.963241923779\n",
      "test auc: 0.964398462471\n",
      "loss at iter 244:0.3083\n",
      "train auc: 0.963290216471\n",
      "test auc: 0.964442369187\n",
      "loss at iter 245:0.3079\n",
      "train auc: 0.963338744167\n",
      "test auc: 0.964487177669\n",
      "loss at iter 246:0.3075\n",
      "train auc: 0.9633870567\n",
      "test auc: 0.964531568108\n",
      "loss at iter 247:0.3070\n",
      "train auc: 0.963434644331\n",
      "test auc: 0.964574781169\n",
      "loss at iter 248:0.3066\n",
      "train auc: 0.963482855003\n",
      "test auc: 0.964619026624\n",
      "loss at iter 249:0.3062\n",
      "train auc: 0.963530316111\n",
      "test auc: 0.964661548796\n",
      "loss at iter 250:0.3058\n",
      "train auc: 0.963578141745\n",
      "test auc: 0.96470552006\n",
      "loss at iter 251:0.3054\n",
      "train auc: 0.963625050358\n",
      "test auc: 0.964747550401\n",
      "loss at iter 252:0.3050\n",
      "train auc: 0.963672531762\n",
      "test auc: 0.964791875324\n",
      "loss at iter 253:0.3046\n",
      "train auc: 0.963719679226\n",
      "test auc: 0.964835494624\n",
      "loss at iter 254:0.3042\n",
      "train auc: 0.963766475962\n",
      "test auc: 0.964878278115\n",
      "loss at iter 255:0.3038\n",
      "train auc: 0.963813384932\n",
      "test auc: 0.964921244217\n",
      "loss at iter 256:0.3035\n",
      "train auc: 0.963859658586\n",
      "test auc: 0.964964368748\n",
      "loss at iter 257:0.3031\n",
      "train auc: 0.963906319268\n",
      "test auc: 0.965006861194\n",
      "loss at iter 258:0.3027\n",
      "train auc: 0.963952191559\n",
      "test auc: 0.965048544283\n",
      "loss at iter 259:0.3023\n",
      "train auc: 0.963998095917\n",
      "test auc: 0.965090715338\n",
      "loss at iter 260:0.3019\n",
      "train auc: 0.964043606819\n",
      "test auc: 0.965133495252\n",
      "loss at iter 261:0.3015\n",
      "train auc: 0.964089089108\n",
      "test auc: 0.965175632791\n",
      "loss at iter 262:0.3011\n",
      "train auc: 0.964134465208\n",
      "test auc: 0.965216913541\n",
      "loss at iter 263:0.3008\n",
      "train auc: 0.964179833128\n",
      "test auc: 0.965258897185\n",
      "loss at iter 264:0.3004\n",
      "train auc: 0.964224350859\n",
      "test auc: 0.965300286049\n",
      "loss at iter 265:0.3000\n",
      "train auc: 0.964268919258\n",
      "test auc: 0.965341696841\n",
      "loss at iter 266:0.2996\n",
      "train auc: 0.964313460916\n",
      "test auc: 0.965380790374\n",
      "loss at iter 267:0.2993\n",
      "train auc: 0.964357824664\n",
      "test auc: 0.965423145541\n",
      "loss at iter 268:0.2989\n",
      "train auc: 0.964401893471\n",
      "test auc: 0.965464733652\n",
      "loss at iter 269:0.2985\n",
      "train auc: 0.964445331528\n",
      "test auc: 0.965505921651\n",
      "loss at iter 270:0.2982\n",
      "train auc: 0.964489094734\n",
      "test auc: 0.965545355898\n",
      "loss at iter 271:0.2978\n",
      "train auc: 0.964532060846\n",
      "test auc: 0.965584047942\n",
      "loss at iter 272:0.2974\n",
      "train auc: 0.964575347106\n",
      "test auc: 0.965625172914\n",
      "loss at iter 273:0.2971\n",
      "train auc: 0.964617926096\n",
      "test auc: 0.965664248932\n",
      "loss at iter 274:0.2967\n",
      "train auc: 0.96466068405\n",
      "test auc: 0.965703481034\n",
      "loss at iter 275:0.2963\n",
      "train auc: 0.964703336148\n",
      "test auc: 0.965741434387\n",
      "loss at iter 276:0.2960\n",
      "train auc: 0.964745724092\n",
      "test auc: 0.965779898092\n",
      "loss at iter 277:0.2956\n",
      "train auc: 0.964788188605\n",
      "test auc: 0.965819355249\n",
      "loss at iter 278:0.2953\n",
      "train auc: 0.964830239201\n",
      "test auc: 0.965857746111\n",
      "loss at iter 279:0.2949\n",
      "train auc: 0.964871835279\n",
      "test auc: 0.965894318579\n",
      "loss at iter 280:0.2946\n",
      "train auc: 0.964913810784\n",
      "test auc: 0.965931493275\n",
      "loss at iter 281:0.2942\n",
      "train auc: 0.964955283878\n",
      "test auc: 0.96596911283\n",
      "loss at iter 282:0.2939\n",
      "train auc: 0.964996922637\n",
      "test auc: 0.966007824169\n",
      "loss at iter 283:0.2935\n",
      "train auc: 0.965038473913\n",
      "test auc: 0.966047351445\n",
      "loss at iter 284:0.2932\n",
      "train auc: 0.965079938651\n",
      "test auc: 0.966085362316\n",
      "loss at iter 285:0.2928\n",
      "train auc: 0.965120842401\n",
      "test auc: 0.966123333277\n",
      "loss at iter 286:0.2925\n",
      "train auc: 0.965161564079\n",
      "test auc: 0.966159919509\n",
      "loss at iter 287:0.2921\n",
      "train auc: 0.965202155559\n",
      "test auc: 0.96619690713\n",
      "loss at iter 288:0.2918\n",
      "train auc: 0.96524278413\n",
      "test auc: 0.966235065684\n",
      "loss at iter 289:0.2914\n",
      "train auc: 0.96528297596\n",
      "test auc: 0.966273489718\n",
      "loss at iter 290:0.2911\n",
      "train auc: 0.965323194508\n",
      "test auc: 0.966310456224\n",
      "loss at iter 291:0.2908\n",
      "train auc: 0.965363252922\n",
      "test auc: 0.966347499995\n",
      "loss at iter 292:0.2904\n",
      "train auc: 0.965403443324\n",
      "test auc: 0.966384722525\n",
      "loss at iter 293:0.2901\n",
      "train auc: 0.96544333666\n",
      "test auc: 0.966421947815\n",
      "loss at iter 294:0.2897\n",
      "train auc: 0.965482970432\n",
      "test auc: 0.966459840824\n",
      "loss at iter 295:0.2894\n",
      "train auc: 0.96552246504\n",
      "test auc: 0.96649807086\n",
      "loss at iter 296:0.2891\n",
      "train auc: 0.965561447441\n",
      "test auc: 0.966535370438\n",
      "loss at iter 297:0.2887\n",
      "train auc: 0.965600219471\n",
      "test auc: 0.966571139981\n",
      "loss at iter 298:0.2884\n",
      "train auc: 0.96563875429\n",
      "test auc: 0.966606483599\n",
      "loss at iter 299:0.2881\n",
      "train auc: 0.9656771803\n",
      "test auc: 0.96664143022\n",
      "loss at iter 300:0.2878\n",
      "train auc: 0.965715592737\n",
      "test auc: 0.966677653103\n",
      "loss at iter 301:0.2874\n",
      "train auc: 0.965753859275\n",
      "test auc: 0.966713891895\n",
      "loss at iter 302:0.2871\n",
      "train auc: 0.965792253255\n",
      "test auc: 0.966749568227\n",
      "loss at iter 303:0.2868\n",
      "train auc: 0.96583095669\n",
      "test auc: 0.966786070117\n",
      "loss at iter 304:0.2865\n",
      "train auc: 0.965869014214\n",
      "test auc: 0.966821362318\n",
      "loss at iter 305:0.2862\n",
      "train auc: 0.965907016507\n",
      "test auc: 0.966856836827\n",
      "loss at iter 306:0.2858\n",
      "train auc: 0.965944976254\n",
      "test auc: 0.966892448255\n",
      "loss at iter 307:0.2855\n",
      "train auc: 0.965983078385\n",
      "test auc: 0.966928314364\n",
      "loss at iter 308:0.2852\n",
      "train auc: 0.966020903379\n",
      "test auc: 0.9669630777\n",
      "loss at iter 309:0.2849\n",
      "train auc: 0.966058724544\n",
      "test auc: 0.966998736742\n",
      "loss at iter 310:0.2846\n",
      "train auc: 0.966096123304\n",
      "test auc: 0.967033039101\n",
      "loss at iter 311:0.2843\n",
      "train auc: 0.966133473163\n",
      "test auc: 0.967067647823\n",
      "loss at iter 312:0.2840\n",
      "train auc: 0.96617039054\n",
      "test auc: 0.967102809748\n",
      "loss at iter 313:0.2837\n",
      "train auc: 0.966207950972\n",
      "test auc: 0.967137675737\n",
      "loss at iter 314:0.2833\n",
      "train auc: 0.96624519477\n",
      "test auc: 0.967172521118\n",
      "loss at iter 315:0.2830\n",
      "train auc: 0.966282018608\n",
      "test auc: 0.967206834368\n",
      "loss at iter 316:0.2827\n",
      "train auc: 0.96631883436\n",
      "test auc: 0.967241381896\n",
      "loss at iter 317:0.2824\n",
      "train auc: 0.96635569596\n",
      "test auc: 0.967275623905\n",
      "loss at iter 318:0.2821\n",
      "train auc: 0.966392191615\n",
      "test auc: 0.96730822136\n",
      "loss at iter 319:0.2818\n",
      "train auc: 0.966428480646\n",
      "test auc: 0.96734184201\n",
      "loss at iter 320:0.2815\n",
      "train auc: 0.966465150712\n",
      "test auc: 0.967375256969\n",
      "loss at iter 321:0.2812\n",
      "train auc: 0.966501531298\n",
      "test auc: 0.967410941569\n",
      "loss at iter 322:0.2809\n",
      "train auc: 0.966537470112\n",
      "test auc: 0.967444410577\n",
      "loss at iter 323:0.2806\n",
      "train auc: 0.966574281022\n",
      "test auc: 0.967477783275\n",
      "loss at iter 324:0.2803\n",
      "train auc: 0.966610673515\n",
      "test auc: 0.967510907722\n",
      "loss at iter 325:0.2800\n",
      "train auc: 0.966646764075\n",
      "test auc: 0.96754538612\n",
      "loss at iter 326:0.2797\n",
      "train auc: 0.966682773464\n",
      "test auc: 0.967578406144\n",
      "loss at iter 327:0.2794\n",
      "train auc: 0.966718480421\n",
      "test auc: 0.967612766908\n",
      "loss at iter 328:0.2792\n",
      "train auc: 0.966754542879\n",
      "test auc: 0.967646992624\n",
      "loss at iter 329:0.2789\n",
      "train auc: 0.966790366784\n",
      "test auc: 0.967679260371\n",
      "loss at iter 330:0.2786\n",
      "train auc: 0.966825599749\n",
      "test auc: 0.967712766664\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at iter 331:0.2783\n",
      "train auc: 0.966861123599\n",
      "test auc: 0.967747462546\n",
      "loss at iter 332:0.2780\n",
      "train auc: 0.96689637385\n",
      "test auc: 0.967780030079\n",
      "loss at iter 333:0.2777\n",
      "train auc: 0.966931438156\n",
      "test auc: 0.967813159394\n",
      "loss at iter 334:0.2774\n",
      "train auc: 0.966966558398\n",
      "test auc: 0.967845493038\n",
      "loss at iter 335:0.2771\n",
      "train auc: 0.967002102891\n",
      "test auc: 0.967879248054\n",
      "loss at iter 336:0.2769\n",
      "train auc: 0.967037045623\n",
      "test auc: 0.967911504443\n",
      "loss at iter 337:0.2766\n",
      "train auc: 0.967072154427\n",
      "test auc: 0.967945332921\n",
      "loss at iter 338:0.2763\n",
      "train auc: 0.967107094579\n",
      "test auc: 0.967978800845\n",
      "loss at iter 339:0.2760\n",
      "train auc: 0.967141821343\n",
      "test auc: 0.968009502259\n",
      "loss at iter 340:0.2757\n",
      "train auc: 0.967176563445\n",
      "test auc: 0.968040688839\n",
      "loss at iter 341:0.2755\n",
      "train auc: 0.967211051275\n",
      "test auc: 0.968073009083\n",
      "loss at iter 342:0.2752\n",
      "train auc: 0.967245267717\n",
      "test auc: 0.968106899266\n",
      "loss at iter 343:0.2749\n",
      "train auc: 0.967279537052\n",
      "test auc: 0.968137721835\n",
      "loss at iter 344:0.2746\n",
      "train auc: 0.96731367614\n",
      "test auc: 0.968169682733\n",
      "loss at iter 345:0.2744\n",
      "train auc: 0.967347656004\n",
      "test auc: 0.968202321733\n",
      "loss at iter 346:0.2741\n",
      "train auc: 0.967381674072\n",
      "test auc: 0.96823512231\n",
      "loss at iter 347:0.2738\n",
      "train auc: 0.967415758316\n",
      "test auc: 0.968267054508\n",
      "loss at iter 348:0.2735\n",
      "train auc: 0.967449566353\n",
      "test auc: 0.968298774733\n",
      "loss at iter 349:0.2733\n",
      "train auc: 0.967482839894\n",
      "test auc: 0.968328934064\n",
      "loss at iter 350:0.2730\n",
      "train auc: 0.967516288127\n",
      "test auc: 0.968359240842\n",
      "loss at iter 351:0.2727\n",
      "train auc: 0.96754900949\n",
      "test auc: 0.968390639361\n",
      "loss at iter 352:0.2725\n",
      "train auc: 0.967581799911\n",
      "test auc: 0.968420996572\n",
      "loss at iter 353:0.2722\n",
      "train auc: 0.967614267764\n",
      "test auc: 0.968452508852\n",
      "loss at iter 354:0.2720\n",
      "train auc: 0.967647192037\n",
      "test auc: 0.9684824347\n",
      "loss at iter 355:0.2717\n",
      "train auc: 0.967679575966\n",
      "test auc: 0.968513585826\n",
      "loss at iter 356:0.2714\n",
      "train auc: 0.967712273633\n",
      "test auc: 0.968542541552\n",
      "loss at iter 357:0.2712\n",
      "train auc: 0.967744828076\n",
      "test auc: 0.968573048215\n",
      "loss at iter 358:0.2709\n",
      "train auc: 0.967777366694\n",
      "test auc: 0.968603878533\n",
      "loss at iter 359:0.2706\n",
      "train auc: 0.967809890421\n",
      "test auc: 0.968634553444\n",
      "loss at iter 360:0.2704\n",
      "train auc: 0.967842352801\n",
      "test auc: 0.968664504792\n",
      "loss at iter 361:0.2701\n",
      "train auc: 0.96787463837\n",
      "test auc: 0.968693085312\n",
      "loss at iter 362:0.2699\n",
      "train auc: 0.967907068141\n",
      "test auc: 0.968723822731\n",
      "loss at iter 363:0.2696\n",
      "train auc: 0.967939749512\n",
      "test auc: 0.968753733648\n",
      "loss at iter 364:0.2693\n",
      "train auc: 0.967971977707\n",
      "test auc: 0.96878328589\n",
      "loss at iter 365:0.2691\n",
      "train auc: 0.968004038927\n",
      "test auc: 0.968812451379\n",
      "loss at iter 366:0.2688\n",
      "train auc: 0.96803608644\n",
      "test auc: 0.968842442533\n",
      "loss at iter 367:0.2686\n",
      "train auc: 0.968067728344\n",
      "test auc: 0.968871566476\n",
      "loss at iter 368:0.2683\n",
      "train auc: 0.968099194573\n",
      "test auc: 0.968900744704\n",
      "loss at iter 369:0.2681\n",
      "train auc: 0.968130440361\n",
      "test auc: 0.968929533044\n",
      "loss at iter 370:0.2678\n",
      "train auc: 0.9681615284\n",
      "test auc: 0.968958061379\n",
      "loss at iter 371:0.2676\n",
      "train auc: 0.968192588765\n",
      "test auc: 0.968988006178\n",
      "loss at iter 372:0.2673\n",
      "train auc: 0.968223431643\n",
      "test auc: 0.969017239381\n",
      "loss at iter 373:0.2671\n",
      "train auc: 0.968254162026\n",
      "test auc: 0.96904527146\n",
      "loss at iter 374:0.2668\n",
      "train auc: 0.968284743603\n",
      "test auc: 0.969072319145\n",
      "loss at iter 375:0.2666\n",
      "train auc: 0.968315496051\n",
      "test auc: 0.969100834356\n",
      "loss at iter 376:0.2663\n",
      "train auc: 0.968345672335\n",
      "test auc: 0.969129906702\n",
      "loss at iter 377:0.2661\n",
      "train auc: 0.968376028864\n",
      "test auc: 0.969157799712\n",
      "loss at iter 378:0.2659\n",
      "train auc: 0.968406096341\n",
      "test auc: 0.969184976997\n",
      "loss at iter 379:0.2656\n",
      "train auc: 0.968435843663\n",
      "test auc: 0.969212020037\n",
      "loss at iter 380:0.2654\n",
      "train auc: 0.968465686322\n",
      "test auc: 0.969239816047\n",
      "loss at iter 381:0.2651\n",
      "train auc: 0.968495326224\n",
      "test auc: 0.969267941777\n",
      "loss at iter 382:0.2649\n",
      "train auc: 0.968524634077\n",
      "test auc: 0.969295318644\n",
      "loss at iter 383:0.2647\n",
      "train auc: 0.968554731189\n",
      "test auc: 0.969323921263\n",
      "loss at iter 384:0.2644\n",
      "train auc: 0.968584920112\n",
      "test auc: 0.969352216552\n",
      "loss at iter 385:0.2642\n",
      "train auc: 0.96861440135\n",
      "test auc: 0.969379411761\n",
      "loss at iter 386:0.2639\n",
      "train auc: 0.968644451454\n",
      "test auc: 0.969405347937\n",
      "loss at iter 387:0.2637\n",
      "train auc: 0.968673513686\n",
      "test auc: 0.969431880253\n",
      "loss at iter 388:0.2635\n",
      "train auc: 0.968703145825\n",
      "test auc: 0.969458752046\n",
      "loss at iter 389:0.2632\n",
      "train auc: 0.968732343885\n",
      "test auc: 0.969486193221\n",
      "loss at iter 390:0.2630\n",
      "train auc: 0.968761907079\n",
      "test auc: 0.969513687462\n",
      "loss at iter 391:0.2628\n",
      "train auc: 0.968791069481\n",
      "test auc: 0.969540408544\n",
      "loss at iter 392:0.2625\n",
      "train auc: 0.968820225031\n",
      "test auc: 0.969566098334\n",
      "loss at iter 393:0.2623\n",
      "train auc: 0.968849704159\n",
      "test auc: 0.969593906919\n",
      "loss at iter 394:0.2621\n",
      "train auc: 0.968878543793\n",
      "test auc: 0.969620352737\n",
      "loss at iter 395:0.2619\n",
      "train auc: 0.968907578292\n",
      "test auc: 0.969646727413\n",
      "loss at iter 396:0.2616\n",
      "train auc: 0.968936582129\n",
      "test auc: 0.969674720619\n",
      "loss at iter 397:0.2614\n",
      "train auc: 0.96896486905\n",
      "test auc: 0.969701498078\n",
      "loss at iter 398:0.2612\n",
      "train auc: 0.96899370144\n",
      "test auc: 0.969728516504\n",
      "loss at iter 399:0.2609\n",
      "train auc: 0.96902241648\n",
      "test auc: 0.969755643569\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "trainloss = list()\n",
    "testloss  = list()\n",
    "acctrain  = list()\n",
    "acctest   = list()\n",
    "s.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "# run optimizing iterations\n",
    "for i in range(400):\n",
    "    #batchX, batchY = s.run(tf.train.batch([X_train_flat, y_train_oh],100,enqueue_many=True, capacity=1))\n",
    "    s.run(optimizer, {input_X: X_train_flat, input_y: y_train_oh})\n",
    "    #s.run(optimizer, {input_X: batchX, input_y: batchY})\n",
    "    loss_i = s.run(loss, {input_X: X_train_flat, input_y: y_train_oh})\n",
    "    trainloss.append(loss_i)\n",
    "    loss_i = s.run(loss, {input_X: X_test_flat, input_y: y_test_oh})\n",
    "    testloss.append(loss_i)\n",
    "    acctrain.append(s.run(accuracy, feed_dict={input_X:X_train_flat, input_y: y_train_oh}))\n",
    "    acctest.append(s.run(accuracy, feed_dict={input_X:X_test_flat, input_y: y_test_oh}))\n",
    "    print(\"loss at iter %i:%.4f\" % (i, loss_i))\n",
    "    print(\"train auc:\", roc_auc_score(y_train_oh, s.run(predicted_y, {input_X:X_train_flat})))\n",
    "    print(\"test auc:\", roc_auc_score(y_test_oh, s.run(predicted_y, {input_X:X_test_flat})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the plots of loss and accuracy on train and test data for each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEdCAYAAAAPT9w1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhAJfRGSM+P8hzzYfuzqCZwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x28e3babdfd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEdCAYAAADjFntmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x28e3bf7cc88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(trainloss, label = \"loss train\")\n",
    "plt.plot(testloss, label=\"loss test\")\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('iteration')\n",
    "plt.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc=3,\n",
    "           ncol=2, mode=\"expand\", borderaxespad=0.)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(acctrain, label = \"accuracy train\")\n",
    "plt.plot(acctest, label  = \"accuracy test\")\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('iteration')\n",
    "plt.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc=3,\n",
    "           ncol=2, mode=\"expand\", borderaxespad=0.)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will print the accuracy (proportion of correct predictions) reached after 400 learning steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on train data: 0.9229\n",
      "Accuracy on val data: 0.9272\n",
      "Accuracy on test data: 0.922\n"
     ]
    }
   ],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(predicted_y,1), tf.argmax(input_y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(\"Accuracy on train data:\", s.run(accuracy, feed_dict={input_X:X_train_flat, input_y: y_train_oh}))\n",
    "print(\"Accuracy on val data:\"  , s.run(accuracy, feed_dict={input_X:X_val_flat, input_y: y_val_oh}))\n",
    "print(\"Accuracy on test data:\" , s.run(accuracy, feed_dict={input_X:X_test_flat, input_y: y_test_oh}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
