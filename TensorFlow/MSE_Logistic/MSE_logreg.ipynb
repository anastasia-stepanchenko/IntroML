{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "s = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1st assignment: MSE\n",
    "\n",
    "To implement mean squared error in tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"MSE\"):\n",
    "    y_true = tf.placeholder(\"float32\", shape=(None,), name=\"y_true\")\n",
    "    y_predicted = tf.placeholder(\"float32\", shape=(None,), name=\"y_predicted\")\n",
    "    mse = tf.reduce_mean((y_true - y_predicted)**2)\n",
    "def compute_mse(vector1, vector2):\n",
    "    return mse.eval({y_true: vector1, y_predicted: vector2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 3] [1, 1, 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.3333334"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dummy = np.arange(5).astype('float32')\n",
    "vector1 = [1,1,3]\n",
    "vector2 = [1,1,1]\n",
    "print(vector1, vector2)\n",
    "compute_mse(vector1,vector2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2nd assignment: Logistic regression\n",
    "To implement the logistic regression\n",
    "\n",
    "Plan:\n",
    "* Use a shared variable for weights\n",
    "* Use a matrix placeholder for `X`\n",
    " \n",
    "We shall train on a two-class MNIST dataset\n",
    "* please note that target `y` are `{0,1}` and not `{-1,1}` as in some formulae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y [shape - (360,)]: [0 1 0 1 0 1 0 0 1 1]\n",
      "X [shape - (360, 64)]:\n",
      "X:\n",
      " [[  0.   0.   5.  13.   9.   1.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.  12.  13.   5.   0.   0.   0.   0.]\n",
      " [  0.   0.   1.   9.  15.  11.   0.   0.   0.   0.]]\n",
      "y:\n",
      " [0 1 0 1 0 1 0 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "mnist = load_digits(2)\n",
    "\n",
    "X, y = mnist.data, mnist.target\n",
    "\n",
    "print(\"y [shape - %s]:\" % (str(y.shape)), y[:10])\n",
    "print(\"X [shape - %s]:\" % (str(X.shape)))\n",
    "print('X:\\n',X[:3,:10])\n",
    "print('y:\\n',y[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's your turn now!\n",
    "Just a small reminder of the relevant math:\n",
    "\n",
    "$$\n",
    "P(y=1|X) = \\sigma(X \\cdot W + b)\n",
    "$$\n",
    "$$\n",
    "\\text{loss} = -\\log\\left(P\\left(y_\\text{predicted} = 1\\right)\\right)\\cdot y_\\text{true} - \\log\\left(1 - P\\left(y_\\text{predicted} = 1\\right)\\right)\\cdot\\left(1 - y_\\text{true}\\right)\n",
    "$$\n",
    "\n",
    "$\\sigma(x)$ is available via `tf.nn.sigmoid` and matrix multiplication via `tf.matmul`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((270, 64), (270,), (90, 64))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, random_state=42)\n",
    "X_train.shape, y_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model parameters - weights and bias\n",
    "weights = tf.Variable(tf.random_normal([X.shape[1],1], stddev=0.35),\n",
    "                      name=\"weights\") \n",
    "b = tf.Variable(0, dtype='float32', name=\"biases\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Variable 'weights:0' shape=(64, 1) dtype=float32_ref>,\n",
       " <tf.Variable 'biases:0' shape=() dtype=float32_ref>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'Placeholder:0' shape=(?, ?) dtype=float32>,\n",
       " <tf.Tensor 'Placeholder_1:0' shape=(?,) dtype=float32>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Placeholders for the input data\n",
    "input_X = tf.placeholder('float32', shape=(None,None))\n",
    "input_y = tf.placeholder('float32', shape=(None, ))\n",
    "input_X, input_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The model code\n",
    "\n",
    "# Compute a vector of predictions, resulting shape should be [input_X.shape[0],]\n",
    "# This is 1D, if you have extra dimensions, you can  get rid of them with tf.squeeze .\n",
    "# Don't forget the sigmoid.\n",
    "a = tf.matmul(input_X, weights)\n",
    "predicted_y =  tf.nn.sigmoid(tf.squeeze(a)+b)\n",
    "\n",
    "# Loss. Should be a scalar number - average loss over all the objects\n",
    "loss = tf.reduce_mean(-tf.log(predicted_y+1e-07)*input_y-tf.log(1-predicted_y+1e-07)*(1 - input_y))\n",
    "\n",
    "# See above for an example. tf.train.*Optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(\n",
    "    loss, var_list=(weights,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'Sigmoid:0' shape=<unknown> dtype=float32>,\n",
       " <tf.Tensor 'Mean:0' shape=<unknown> dtype=float32>,\n",
       " <tf.Operation 'GradientDescent' type=NoOp>,\n",
       " <tf.Tensor 'MatMul:0' shape=(?, 1) dtype=float32>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_y, loss, optimizer, a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A test to help with the debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "my_scalar = tf.placeholder('float32')\n",
    "my_vector = tf.placeholder('float32', [None])\n",
    "\n",
    "# Warning! Trying to understand the meaning of that function may result in permanent brain damage\n",
    "weird_psychotic_function = tf.reduce_mean(\n",
    "    (my_vector+my_scalar)**(1+tf.nn.moments(my_vector,[0])[1]) + \n",
    "    1./ tf.atan(my_scalar))/(my_scalar**2 + 1) + 0.01*tf.sin(\n",
    "    2*my_scalar**1.5)*(tf.reduce_sum(my_vector)* my_scalar**2\n",
    "                      )*tf.exp((my_scalar-4)**2)/(\n",
    "    1+tf.exp((my_scalar-4)**2))*(1.-(tf.exp(-(my_scalar-4)**2)\n",
    "                                    )/(1+tf.exp(-(my_scalar-4)**2)))**2\n",
    "\n",
    "validation_weights = 1e-3 * np.fromiter(map(lambda x:\n",
    "        s.run(weird_psychotic_function, {my_scalar:x, my_vector:[1, 0.1, 2]}),\n",
    "                                   0.15 * np.arange(1, X.shape[1] + 1)),\n",
    "                                   count=X.shape[1], dtype=np.float32)[:, np.newaxis]\n",
    "\n",
    "# Compute predictions for given weights and bias\n",
    "prediction_validation = s.run(\n",
    "    predicted_y, {\n",
    "    input_X: X,\n",
    "    weights: validation_weights,\n",
    "    b: 1e-1})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True,\n",
       " 'Predictions must be a 1D array with length equal to the number of examples in input_X')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the reference values for the predictions\n",
    "validation_true_values = np.loadtxt(\"D:\\\\Downloads\\\\validation_predictons.txt\"),\n",
    "prediction_validation.shape == (X.shape[0],), \\\n",
    "       \"Predictions must be a 1D array with length equal to the number \" \\\n",
    "       \"of examples in input_X\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(validation_true_values, prediction_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_validation = s.run(\n",
    "        loss, {\n",
    "            input_X: X[:100],\n",
    "            input_y: y[-100:],\n",
    "            weights: validation_weights+1.21e-3,\n",
    "            b: -1e-1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(loss_validation, 0.728689)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at iter 0:1.7502\n",
      "train auc: 0.997200263505\n",
      "test auc: 0.997529644269\n",
      "loss at iter 1:3.2488\n",
      "train auc: 0.92803030303\n",
      "test auc: 0.945652173913\n",
      "loss at iter 2:3.1453\n",
      "train auc: 1.0\n",
      "test auc: 1.0\n",
      "loss at iter 3:0.0784\n",
      "train auc: 1.0\n",
      "test auc: 1.0\n",
      "loss at iter 4:0.0349\n",
      "train auc: 1.0\n",
      "test auc: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "s.run(tf.global_variables_initializer())\n",
    "for i in range(5):\n",
    "    s.run(optimizer, {input_X: X_train, input_y: y_train})\n",
    "    loss_i = s.run(loss, {input_X: X_train, input_y: y_train})\n",
    "    print(\"loss at iter %i:%.4f\" % (i, loss_i))\n",
    "    print(\"train auc:\", roc_auc_score(y_train, s.run(predicted_y, {input_X:X_train})))\n",
    "    print(\"test auc:\", roc_auc_score(y_test, s.run(predicted_y, {input_X:X_test})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_weights = 1e-3 * np.fromiter(map(lambda x:\n",
    "    s.run(weird_psychotic_function, {my_scalar:x, my_vector:[1, 2, 3]}),\n",
    "                               0.1 * np.arange(1, X.shape[1] + 1)),\n",
    "                               count=X.shape[1], dtype=np.float32)[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, test prediction and loss computation. This part doesn't require a fitted model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction_test = s.run(\n",
    "    predicted_y, {\n",
    "    input_X: X,\n",
    "    weights: test_weights,\n",
    "    b: 1e-1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True,\n",
       " 'Predictions must be a 1D array with length equal to the number of examples in X_test')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_test.shape == (X.shape[0],),\\\n",
    "       \"Predictions must be a 1D array with length equal to the number \" \\\n",
    "       \"of examples in X_test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_test = s.run(\n",
    "    loss, {\n",
    "        input_X: X[:100],\n",
    "        input_y: y[-100:],\n",
    "        weights: test_weights+1.21e-3,\n",
    "        b: -1e-1})\n",
    "# Yes, the X/y indices mistmach is intentional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.77501142, 1.0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_test, roc_auc_score(y_test, s.run(predicted_y, {input_X:X_test}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
